{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - imports, constants, download of data from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 11:46:17.538950: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-10 11:46:18.274094: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-10 11:46:19.725143: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-10 11:46:19.725213: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-10 11:46:19.725220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as tfm\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow.keras.callbacks as tfc\n",
    "import tensorflow.keras.utils as tfu\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import functools\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "\n",
    "os.system(\"pip install keras-flops > /dev/null\")\n",
    "import keras_flops as kf\n",
    "\n",
    "SEED = 42\n",
    "IMAGE_HEIGHT = 20\n",
    "IMAGE_WIDTH = 20\n",
    "SAMPLES_PER_MEASUREMENT = 119\n",
    "LINES_PER_MEASUREMENT = SAMPLES_PER_MEASUREMENT + 1\n",
    "IMAGE_WIDTH_HEIGHT_INDEX = IMAGE_WIDTH - 1\n",
    "SAMPLES_PER_MEASUREMENT_CROPPED = 110\n",
    "FRONT_CROP = 2\n",
    "BACK_CROP = 7\n",
    "NUMBER_OF_LABELS = 5\n",
    "LABELS = [\"Alohomora\", \"Arresto Momentum\", \"Avada Kedavra\", \"Locomotor\", \"Revelio\"]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "LARGE_DATA_SET = False  # set to True to train on larger data set\n",
    "POOR_MODELS = False     # set to True to train also model, for which training fails\n",
    "FULL_INPUT = False      # set to True to train on full 119 samples of a spell, otherwise start and end are scopped to 110 samples\n",
    "\n",
    "# create input/output directories and download data\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"data_large\", exist_ok=True)\n",
    "\n",
    "if not os.path.isfile(\"data/spells.zip\") and not LARGE_DATA_SET:\n",
    "    os.system(\"wget -P data/ https://github.com/xmihol00/robust_magic_wand/raw/main/data/spells.zip\")\n",
    "    os.system(\"unzip data/spells.zip\")\n",
    "\n",
    "if not os.path.isfile(\"data_large/spells.zip\") and LARGE_DATA_SET:\n",
    "    os.system(\"wget -P data_large/ https://github.com/xmihol00/robust_magic_wand/raw/main/data_large/spells.zip\")\n",
    "    os.system(\"unzip data_large/spells.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing, creation of data sets and model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stroke_samples(data):\n",
    "    angle_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 3))\n",
    "    stroke_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 2))\n",
    "    rows_of_samples = [list(map(lambda x: float(x), line.split(','))) for line in data.split('\\n') if line]\n",
    "\n",
    "    for i in range(0, len(rows_of_samples), SAMPLES_PER_MEASUREMENT): \n",
    "        measurment = np.array(rows_of_samples[i: i+SAMPLES_PER_MEASUREMENT])\n",
    "        acceleration_average = np.average(measurment[:, 0:3], axis=0)\n",
    "\n",
    "        # calcualte angle\n",
    "        previous_angle = np.zeros(3)\n",
    "        for j, gyro_sample in enumerate(measurment[:, 3:6]):\n",
    "            angle_samples[j] = previous_angle + gyro_sample / SAMPLES_PER_MEASUREMENT\n",
    "            previous_angle = angle_samples[j]     \n",
    "        angle_avg = np.average(angle_samples, axis=0) # average angle\n",
    "\n",
    "        # calculate stroke\n",
    "        acceleration_magnitude = np.sqrt(acceleration_average.dot(acceleration_average.T)) # dot product insted of squaring\n",
    "        acceleration_magnitude += (acceleration_magnitude < 0.0001) * 0.0001 # prevent division by 0\n",
    "        normalzied_acceleration = acceleration_average / acceleration_magnitude\n",
    "        normalized_angle = angle_samples - angle_avg\n",
    "        stroke_samples[:, 0] = -normalzied_acceleration[1] * normalized_angle[:, 1] - normalzied_acceleration[2] * normalized_angle[:, 2]\n",
    "        stroke_samples[:, 1] =  normalzied_acceleration[1] * normalized_angle[:, 2] - normalzied_acceleration[2] * normalized_angle[:, 1]\n",
    "        yield stroke_samples\n",
    "\n",
    "def laod_dataset_as_arrays_and_images(directory, one_hot=True, seed=SEED):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    for i, file_name in enumerate(sorted(glob.glob(f\"{directory}/*.csv\"))): # sort the labels alphabetically\n",
    "        file = open(file_name, \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    arrays = np.zeros((len(labels), 2 * SAMPLES_PER_MEASUREMENT), dtype=np.float32)\n",
    "    colors = np.linspace(255 - 2 * SAMPLES_PER_MEASUREMENT + 2, 255, SAMPLES_PER_MEASUREMENT) / 255\n",
    "    images = np.zeros((len(labels), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        stroke_samples /= np.max(stroke_samples, axis=0) # normalize values from 0 to 1\n",
    "        arrays[i] = stroke_samples.reshape(-1)\n",
    "        \n",
    "        # rasterize stroke\n",
    "        pixels = np.round(stroke_samples * IMAGE_WIDTH_HEIGHT_INDEX, 0).astype(np.uint8) # normalize samples to the whole image\n",
    "        image = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image[pixels[:, 1], pixels[:, 0]] = colors\n",
    "        images[i] = image.reshape(IMAGE_WIDTH, IMAGE_HEIGHT, 1).astype(np.float32)\n",
    "\n",
    "\n",
    "    both_variants = [skms.train_test_split(arrays, labels, test_size=0.2, random_state=seed),\n",
    "                     skms.train_test_split(images, labels, test_size=0.2, random_state=seed)]\n",
    "\n",
    "    for i in range(len(both_variants)):\n",
    "        X_train, X_test, y_train, y_test = both_variants[i]\n",
    "        y_test = np.array(y_test)\n",
    "        if one_hot:\n",
    "            # one-hot encoding of labels\n",
    "            y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "        both_variants[i] = (X_train, X_test, y_train, y_test)\n",
    "       \n",
    "    return both_variants\n",
    "\n",
    "def laod_dataset_as_arrays_and_images_cropped(directory, one_hot=True, seed=SEED):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    for i, file_name in enumerate(sorted(glob.glob(f\"{directory}/*.csv\"))):\n",
    "        file = open(file_name, \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    arrays = np.zeros((len(labels), 2 * SAMPLES_PER_MEASUREMENT_CROPPED), dtype=np.float32)\n",
    "    colors = np.linspace(255 - 2 * SAMPLES_PER_MEASUREMENT_CROPPED + 2, 255, SAMPLES_PER_MEASUREMENT_CROPPED) / 255\n",
    "    images = np.zeros((len(labels), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        stroke_samples /= np.max(stroke_samples, axis=0) # normalize values from 0 to 1\n",
    "        stroke_samples = stroke_samples[FRONT_CROP:-BACK_CROP, :]\n",
    "        arrays[i] = stroke_samples.reshape(-1)\n",
    "        \n",
    "        # rasterize stroke\n",
    "        pixels = np.round(stroke_samples * IMAGE_WIDTH_HEIGHT_INDEX, 0).astype(np.uint8) # normalize samples to the whole image\n",
    "        image = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image[pixels[:, 1], pixels[:, 0]] = colors\n",
    "        images[i] = image.reshape(IMAGE_WIDTH, IMAGE_HEIGHT, 1).astype(np.float32)\n",
    "\n",
    "\n",
    "    both_variants = [skms.train_test_split(arrays, labels, test_size=0.2, random_state=seed),\n",
    "                     skms.train_test_split(images, labels, test_size=0.2, random_state=seed)]\n",
    "\n",
    "    for i in range(len(both_variants)):\n",
    "        X_train, X_test, y_train, y_test = both_variants[i]\n",
    "        y_test = np.array(y_test)\n",
    "        if one_hot:\n",
    "            # one-hot encoding of labels\n",
    "            y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "        both_variants[i] = (X_train, X_test, y_train, y_test)\n",
    "       \n",
    "    return both_variants\n",
    "\n",
    "def representative_dataset(data_set):\n",
    "    for sample in data_set:\n",
    "        yield [np.expand_dims(sample, 0)]\n",
    "\n",
    "def collect_model_summary(summary_line, model_dict):\n",
    "    match = re.match(r\"(.*?): ([\\d,]+)\", summary_line)\n",
    "    if match:\n",
    "        match = match.groups()\n",
    "        model_dict[match[0].replace(\"params\", \"parameters\")] = int(match[1].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_activation = tf.keras.layers.LeakyReLU(0.1)\n",
    "droput_1 = 0.4\n",
    "droput_2 = 0.3\n",
    "droput_3 = 0.25\n",
    "\n",
    "models = [\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=125, activation=hidden_activation),\n",
    "        tfl.Dense(units=75, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=32, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=32, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=128, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=125, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=75, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=32, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=32, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=128, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=32, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"baseline_linear\",\n",
    "    \"Only_DENS_S\", \"Only_DENS_L\",\n",
    "    \"CONV_DENS_S\", \"CONV_DENS_L\", \n",
    "    \"Only_CONV_S\", \"Only_CONV_L\",\n",
    "    \"Only_DENS_DO_S\", \"Only_DENS_DO_L\",\n",
    "    \"CONV_DENS_DO_S\", \"CONV_DENS_DO_L\",\n",
    "    \"Only_CONV_BN_S\", \"Only_CONV_BN_L\",\n",
    "    \"CONV_DENS_BN_DO_S\", \"CONV_DENS_BN_DO_L\",\n",
    "]\n",
    "\n",
    "model_data_sets = [\n",
    "    0,\n",
    "    0, 0,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "    0, 0,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "    1, 1\n",
    "]\n",
    "\n",
    "selected_models = [ \"Only_DENS_DO_L\", \"Only_CONV_L\" ]\n",
    "\n",
    "if not POOR_MODELS:\n",
    "    models = models[:-4]\n",
    "    model_names = model_names[:-4]\n",
    "    model_data_sets = model_data_sets[:-4]\n",
    "\n",
    "\n",
    "table_header = [\"Total parameters\", \"Trainable parameters\", \"Non-trainable parameters\", \"Size\", \"Optimized size\", \"Training time GPU\", \n",
    "                \"Epochs\", \"FLOPS\", \"Full model accuracy\", \"Optimized model accuracy\", \"Full model inference time\", \"Optimized model inference time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data sets and get the order of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of labels:\n",
      "    Alohomora\n",
      "    Aresto Momentum\n",
      "    Avada Kedavra\n",
      "    Locomotor\n",
      "    Revelio\n"
     ]
    }
   ],
   "source": [
    "if FULL_INPUT:\n",
    "    data_sets = laod_dataset_as_arrays_and_images(\"data_large\" if LARGE_DATA_SET else \"data\")\n",
    "else:\n",
    "    data_sets = laod_dataset_as_arrays_and_images_cropped(\"data_large\" if LARGE_DATA_SET else \"data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and collection of statistics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "figure, axis = plt.subplots(2, 2) # plot of the confusion matrix for selected models\n",
    "figure.set_size_inches(22, 18)\n",
    "axis_idx = 0\n",
    "plt.subplots_adjust(left=0.085, bottom=0.04, right=1.02, top=0.97, hspace=0.15, wspace=0.15)\n",
    "\n",
    "for model, model_name, data_set in zip(models, model_names, model_data_sets):\n",
    "    X_train, X_test, y_train, y_test = data_sets[data_set]\n",
    "    results[model_name] = {}\n",
    "    results_model = results[model_name]\n",
    "    results_model[\"Full model inference time\"] = \"-----\"\n",
    "    results_model[\"Optimized model inference time\"] = \"-----\"\n",
    "\n",
    "    # reset the seeds, so all models have the same starting point\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    # get weights for the given seed\n",
    "    model.build(X_train.shape)\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    # get the best number of epochs based on validation data set\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=2,\n",
    "                        callbacks=[tfc.EarlyStopping(monitor=\"val_accuracy\", patience=3, mode=\"max\", restore_best_weights=False)]).history\n",
    "    model.set_weights(weights)\n",
    "    epochs = len(history[\"loss\"]) - 3\n",
    "    results_model[\"Epochs\"] = epochs\n",
    "    \n",
    "    # train on the whole train data set\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    train_start = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.0, batch_size=32, verbose=2)\n",
    "    results_model[\"Training time GPU\"] = f\"{(time.time() - train_start):.2f} s\"\n",
    "\n",
    "    # predict and evaluate the prediction\n",
    "    predictions = model.predict(X_test, verbose=2)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    results_model[\"Full model accuracy\"] = f\"{((predictions == y_test).sum() / y_test.shape[0] * 100):.2f} \\\\%\"\n",
    "\n",
    "    # plot the full model confusion metrix\n",
    "    if model_name in selected_models:\n",
    "        confusion_matrix = tf.math.confusion_matrix(y_test, predictions).numpy()\n",
    "        confusion_matrix = skm.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=LABELS)\n",
    "        axis[axis_idx // 2, axis_idx % 2].set_title(f\"{model_name} unoptimized\")\n",
    "        confusion_matrix.plot(cmap=\"Blues\", ax=axis[axis_idx // 2, axis_idx % 2])\n",
    "        axis_idx += 1\n",
    "\n",
    "    # get the summary of the model\n",
    "    model.summary(print_fn=lambda x, y=results_model: collect_model_summary(x, y))\n",
    "    results_model[\"FLOPS\"] = kf.get_flops(model, batch_size=1)\n",
    "\n",
    "    # convert the model without optimiziation (evaluation is not necessary, the results after conversion are the same)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    results_file = open(f\"models/{model_name}.tflite\", \"wb\")\n",
    "    results_file.write(tflite_model)\n",
    "    results_file.close()\n",
    "    results_model[\"Size\"] = os.path.getsize(f\"models/{model_name}.tflite\")\n",
    "    os.system(f'echo \"const unsigned char model[] = {{\" > models/{model_name}.h && cat models/{model_name}.tflite | xxd -i >> models/{model_name}.h && echo \"}};\" >> models/{model_name}.h && rm -f models/{model_name}.tflite')\n",
    "    del tflite_model\n",
    "\n",
    "    # convert the model with optimization \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    converter.representative_dataset = lambda x=X_train: representative_dataset(x)\n",
    "    tflite_model_opt = converter.convert()\n",
    "    results_file = open(f\"models/{model_name}.tflite\", \"wb\")\n",
    "    results_file.write(tflite_model_opt)\n",
    "    results_file.close()\n",
    "    results_model[\"Optimized size\"] = os.path.getsize(f\"models/{model_name}.tflite\")\n",
    "    os.system(f'echo \"const unsigned char model[] = {{\" > models/{model_name}_opt.h && cat models/{model_name}.tflite | xxd -i >> models/{model_name}_opt.h && echo \"}};\" >> models/{model_name}_opt.h && rm -f models/{model_name}.tflite')\n",
    "\n",
    "    # predict using the optimized model and evaluate the prediction\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model_opt)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    input_scale, input_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "    predictions = np.zeros((y_test.shape[0]))\n",
    "    for i, sample in enumerate(X_test):\n",
    "        interpreter.set_tensor(input_index, np.expand_dims(sample / input_scale + input_zero_point, 0).astype(np.int8))\n",
    "        interpreter.invoke()\n",
    "        predictions[i] = np.argmax(interpreter.get_tensor(output_index)[0]) # rescaling is not needed\n",
    "    results_model[\"Optimized model accuracy\"] = f\"{((predictions == y_test).sum() / y_test.shape[0] * 100):.2f} \\\\%\"\n",
    "    \n",
    "    # plot the confusuion matrix of the optimized model\n",
    "    if model_name in selected_models:\n",
    "        confusion_matrix = tf.math.confusion_matrix(y_test, predictions).numpy()\n",
    "        confusion_matrix = skm.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=LABELS)\n",
    "        axis[axis_idx // 2, axis_idx % 2].set_title(f\"{model_name} optimized\")\n",
    "        confusion_matrix.plot(cmap=\"Blues\", ax=axis[axis_idx // 2, axis_idx % 2])\n",
    "        axis_idx += 1\n",
    "    \n",
    "    del tflite_model_opt\n",
    "\n",
    "    # clear cell output after each model\n",
    "    ipd.clear_output()\n",
    "\n",
    "plt.savefig(f\"figures/confusion_matrix.png\", dpi=300)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total parameters</th>\n",
       "      <th>Trainable parameters</th>\n",
       "      <th>Non-trainable parameters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Optimized size</th>\n",
       "      <th>Training time GPU</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>FLOPS</th>\n",
       "      <th>Full model accuracy</th>\n",
       "      <th>Optimized model accuracy</th>\n",
       "      <th>Inference time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_linear</th>\n",
       "      <td>1105.0</td>\n",
       "      <td>1105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5740.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0.57 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>86.00 %</td>\n",
       "      <td>85.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_S</th>\n",
       "      <td>22605.0</td>\n",
       "      <td>22605.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92320.0</td>\n",
       "      <td>25168.0</td>\n",
       "      <td>0.39 s</td>\n",
       "      <td>6.0</td>\n",
       "      <td>45130.0</td>\n",
       "      <td>89.00 %</td>\n",
       "      <td>87.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_L</th>\n",
       "      <td>37455.0</td>\n",
       "      <td>37455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152284.0</td>\n",
       "      <td>41024.0</td>\n",
       "      <td>0.49 s</td>\n",
       "      <td>9.0</td>\n",
       "      <td>74730.0</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_S</th>\n",
       "      <td>10181.0</td>\n",
       "      <td>10181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45564.0</td>\n",
       "      <td>17472.0</td>\n",
       "      <td>2.47 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>537886.0</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_L</th>\n",
       "      <td>40069.0</td>\n",
       "      <td>40069.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165116.0</td>\n",
       "      <td>48968.0</td>\n",
       "      <td>1.52 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2013726.0</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_S</th>\n",
       "      <td>23877.0</td>\n",
       "      <td>23877.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100432.0</td>\n",
       "      <td>32584.0</td>\n",
       "      <td>1.69 s</td>\n",
       "      <td>7.0</td>\n",
       "      <td>584670.0</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_L</th>\n",
       "      <td>93829.0</td>\n",
       "      <td>93829.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380240.0</td>\n",
       "      <td>105560.0</td>\n",
       "      <td>1.03 s</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1906590.0</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>97.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_DO_S</th>\n",
       "      <td>22605.0</td>\n",
       "      <td>22605.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92320.0</td>\n",
       "      <td>25168.0</td>\n",
       "      <td>0.49 s</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45130.0</td>\n",
       "      <td>91.00 %</td>\n",
       "      <td>91.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_DO_L</th>\n",
       "      <td>37455.0</td>\n",
       "      <td>37455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152284.0</td>\n",
       "      <td>41024.0</td>\n",
       "      <td>0.64 s</td>\n",
       "      <td>7.0</td>\n",
       "      <td>74730.0</td>\n",
       "      <td>92.00 %</td>\n",
       "      <td>91.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_DO_S</th>\n",
       "      <td>10181.0</td>\n",
       "      <td>10181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45564.0</td>\n",
       "      <td>17472.0</td>\n",
       "      <td>1.29 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>537886.0</td>\n",
       "      <td>98.00 %</td>\n",
       "      <td>98.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_DO_L</th>\n",
       "      <td>40069.0</td>\n",
       "      <td>40069.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165116.0</td>\n",
       "      <td>48968.0</td>\n",
       "      <td>1.10 s</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2013726.0</td>\n",
       "      <td>99.00 %</td>\n",
       "      <td>97.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_BN_S</th>\n",
       "      <td>24325.0</td>\n",
       "      <td>24101.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>103224.0</td>\n",
       "      <td>35136.0</td>\n",
       "      <td>0.66 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>595966.0</td>\n",
       "      <td>60.00 %</td>\n",
       "      <td>57.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_BN_L</th>\n",
       "      <td>94725.0</td>\n",
       "      <td>94277.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>383928.0</td>\n",
       "      <td>108336.0</td>\n",
       "      <td>1.24 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1929182.0</td>\n",
       "      <td>62.00 %</td>\n",
       "      <td>64.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_BN_DO_S</th>\n",
       "      <td>10405.0</td>\n",
       "      <td>10293.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>47912.0</td>\n",
       "      <td>19920.0</td>\n",
       "      <td>1.17 s</td>\n",
       "      <td>3.0</td>\n",
       "      <td>549422.0</td>\n",
       "      <td>57.00 %</td>\n",
       "      <td>56.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_BN_DO_L</th>\n",
       "      <td>40517.0</td>\n",
       "      <td>40293.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>167912.0</td>\n",
       "      <td>51528.0</td>\n",
       "      <td>0.69 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2036798.0</td>\n",
       "      <td>51.00 %</td>\n",
       "      <td>46.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Total parameters  Trainable parameters  \\\n",
       "baseline_linear              1105.0                1105.0   \n",
       "Only_DENS_S                 22605.0               22605.0   \n",
       "Only_DENS_L                 37455.0               37455.0   \n",
       "CONV_DENS_S                 10181.0               10181.0   \n",
       "CONV_DENS_L                 40069.0               40069.0   \n",
       "Only_CONV_S                 23877.0               23877.0   \n",
       "Only_CONV_L                 93829.0               93829.0   \n",
       "Only_DENS_DO_S              22605.0               22605.0   \n",
       "Only_DENS_DO_L              37455.0               37455.0   \n",
       "CONV_DENS_DO_S              10181.0               10181.0   \n",
       "CONV_DENS_DO_L              40069.0               40069.0   \n",
       "Only_CONV_BN_S              24325.0               24101.0   \n",
       "Only_CONV_BN_L              94725.0               94277.0   \n",
       "CONV_DENS_BN_DO_S           10405.0               10293.0   \n",
       "CONV_DENS_BN_DO_L           40517.0               40293.0   \n",
       "\n",
       "                   Non-trainable parameters      Size  Optimized size  \\\n",
       "baseline_linear                         0.0    5740.0          2640.0   \n",
       "Only_DENS_S                             0.0   92320.0         25168.0   \n",
       "Only_DENS_L                             0.0  152284.0         41024.0   \n",
       "CONV_DENS_S                             0.0   45564.0         17472.0   \n",
       "CONV_DENS_L                             0.0  165116.0         48968.0   \n",
       "Only_CONV_S                             0.0  100432.0         32584.0   \n",
       "Only_CONV_L                             0.0  380240.0        105560.0   \n",
       "Only_DENS_DO_S                          0.0   92320.0         25168.0   \n",
       "Only_DENS_DO_L                          0.0  152284.0         41024.0   \n",
       "CONV_DENS_DO_S                          0.0   45564.0         17472.0   \n",
       "CONV_DENS_DO_L                          0.0  165116.0         48968.0   \n",
       "Only_CONV_BN_S                        224.0  103224.0         35136.0   \n",
       "Only_CONV_BN_L                        448.0  383928.0        108336.0   \n",
       "CONV_DENS_BN_DO_S                     112.0   47912.0         19920.0   \n",
       "CONV_DENS_BN_DO_L                     224.0  167912.0         51528.0   \n",
       "\n",
       "                  Training time GPU  Epochs      FLOPS Full model accuracy  \\\n",
       "baseline_linear              0.57 s    12.0     2230.0             86.00 %   \n",
       "Only_DENS_S                  0.39 s     6.0    45130.0             89.00 %   \n",
       "Only_DENS_L                  0.49 s     9.0    74730.0             94.00 %   \n",
       "CONV_DENS_S                  2.47 s    12.0   537886.0             95.00 %   \n",
       "CONV_DENS_L                  1.52 s    12.0  2013726.0            100.00 %   \n",
       "Only_CONV_S                  1.69 s     7.0   584670.0             94.00 %   \n",
       "Only_CONV_L                  1.03 s     7.0  1906590.0             95.00 %   \n",
       "Only_DENS_DO_S               0.49 s    10.0    45130.0             91.00 %   \n",
       "Only_DENS_DO_L               0.64 s     7.0    74730.0             92.00 %   \n",
       "CONV_DENS_DO_S               1.29 s    12.0   537886.0             98.00 %   \n",
       "CONV_DENS_DO_L               1.10 s     7.0  2013726.0             99.00 %   \n",
       "Only_CONV_BN_S               0.66 s     1.0   595966.0             60.00 %   \n",
       "Only_CONV_BN_L               1.24 s     1.0  1929182.0             62.00 %   \n",
       "CONV_DENS_BN_DO_S            1.17 s     3.0   549422.0             57.00 %   \n",
       "CONV_DENS_BN_DO_L            0.69 s     1.0  2036798.0             51.00 %   \n",
       "\n",
       "                  Optimized model accuracy Inference time  \n",
       "baseline_linear                    85.00 %          -----  \n",
       "Only_DENS_S                        87.00 %          -----  \n",
       "Only_DENS_L                        94.00 %          -----  \n",
       "CONV_DENS_S                        95.00 %          -----  \n",
       "CONV_DENS_L                       100.00 %          -----  \n",
       "Only_CONV_S                        94.00 %          -----  \n",
       "Only_CONV_L                        97.00 %          -----  \n",
       "Only_DENS_DO_S                     91.00 %          -----  \n",
       "Only_DENS_DO_L                     91.00 %          -----  \n",
       "CONV_DENS_DO_S                     98.00 %          -----  \n",
       "CONV_DENS_DO_L                     97.00 %          -----  \n",
       "Only_CONV_BN_S                     57.00 %          -----  \n",
       "Only_CONV_BN_L                     64.00 %          -----  \n",
       "CONV_DENS_BN_DO_S                  56.00 %          -----  \n",
       "CONV_DENS_BN_DO_L                  46.00 %          -----  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export colected statistics to LaTex table and pandas data frame\n",
    "data_frame = pd.DataFrame()\n",
    "with open(\"results/statistics_large.tex\" if LARGE_DATA_SET else \"results/statistics.tex\", \"w\") as results_file:\n",
    "    print_rf = functools.partial(print, file=results_file)\n",
    "    row_end = \"\\\\\\\\\"\n",
    "    backslash_underscore = \"\\\\_\"\n",
    "    print_rf(\"\\\\begin{table}[ht]\", \"\\\\begin{adjustwidth}{-0.85cm}{}\" \"\\\\tiny\", \"\\\\center\", \"\\\\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|c|c| }\", sep=\"\\n\")        \n",
    "    print_rf(\"\\\\hline\")\n",
    "\n",
    "    print_rf(\"& \", end=\"\")\n",
    "    for header in table_header[:-1]:\n",
    "        print_rf(f\"\\\\thead{{{header.replace('-', ' ').replace(' ', row_end)}}} & \", end=\"\")\n",
    "    print_rf(f\"\\\\thead{{{table_header[-1].replace(' ', row_end)}}} {row_end}\")\n",
    "    print_rf(\"\\\\hline\")\n",
    "\n",
    "    for model_name in model_names:\n",
    "        results_model = results[model_name]\n",
    "        print_rf(f\"\\\\thead{{{model_name.replace('_', backslash_underscore)}}} & \", end=\"\")\n",
    "        for header in table_header[:-1]:\n",
    "            print_rf(f\"{results_model[header]} & \", end=\"\")\n",
    "            data_frame.at[model_name, header] = results_model[header]\n",
    "        print_rf(f\"{results_model[table_header[-1]]} {row_end}\")\n",
    "        data_frame.at[model_name, table_header[-1]] = results_model[table_header[-1]]\n",
    "\n",
    "    print_rf(\"\\\\hline\")\n",
    "    print_rf(\"\\\\end{tabular}\")\n",
    "    print_rf(\"\\\\caption{Summary of analyzed models during model selection and hyperparametr tuning.}\")\n",
    "    print_rf(\"\\\\label{table1}\")\n",
    "    print_rf(\"\\\\end{adjustwidth}\", \"\\\\end{table}\", sep='\\n')\n",
    "\n",
    "data_frame.replace(\"\\\\\\%\", '%', regex=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
