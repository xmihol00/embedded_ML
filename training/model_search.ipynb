{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total parameters</th>\n",
       "      <th>Trainable parameters</th>\n",
       "      <th>Non-trainable parameters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Optimized size</th>\n",
       "      <th>Training time GPU</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>FLOPS</th>\n",
       "      <th>Full model accuracy</th>\n",
       "      <th>Optimized model accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_linear</th>\n",
       "      <td>1195.0</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6104.0</td>\n",
       "      <td>2736.0</td>\n",
       "      <td>0.30 s</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2410.0</td>\n",
       "      <td>80.77 %</td>\n",
       "      <td>79.23 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_DENS_S</th>\n",
       "      <td>24405.0</td>\n",
       "      <td>24405.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99532.0</td>\n",
       "      <td>26968.0</td>\n",
       "      <td>0.57 s</td>\n",
       "      <td>15.0</td>\n",
       "      <td>48730.0</td>\n",
       "      <td>96.15 %</td>\n",
       "      <td>96.92 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_DENS_M</th>\n",
       "      <td>39705.0</td>\n",
       "      <td>39705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161304.0</td>\n",
       "      <td>43280.0</td>\n",
       "      <td>0.53 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>79230.0</td>\n",
       "      <td>96.92 %</td>\n",
       "      <td>96.92 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_DENS_L</th>\n",
       "      <td>56255.0</td>\n",
       "      <td>56255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228076.0</td>\n",
       "      <td>60832.0</td>\n",
       "      <td>0.53 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>112230.0</td>\n",
       "      <td>97.69 %</td>\n",
       "      <td>97.69 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_1_S</th>\n",
       "      <td>57477.0</td>\n",
       "      <td>57477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234828.0</td>\n",
       "      <td>64888.0</td>\n",
       "      <td>1.78 s</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2221534.0</td>\n",
       "      <td>93.85 %</td>\n",
       "      <td>95.38 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_1_L</th>\n",
       "      <td>228869.0</td>\n",
       "      <td>228869.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>920396.0</td>\n",
       "      <td>237984.0</td>\n",
       "      <td>1.07 s</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8334238.0</td>\n",
       "      <td>96.92 %</td>\n",
       "      <td>97.69 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_2_S</th>\n",
       "      <td>80133.0</td>\n",
       "      <td>80133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325264.0</td>\n",
       "      <td>87328.0</td>\n",
       "      <td>0.66 s</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1618270.0</td>\n",
       "      <td>94.62 %</td>\n",
       "      <td>95.38 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_2_L</th>\n",
       "      <td>319237.0</td>\n",
       "      <td>319237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1281680.0</td>\n",
       "      <td>328136.0</td>\n",
       "      <td>1.00 s</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5374622.0</td>\n",
       "      <td>96.92 %</td>\n",
       "      <td>96.92 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_CONV_S</th>\n",
       "      <td>26757.0</td>\n",
       "      <td>26757.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113628.0</td>\n",
       "      <td>38424.0</td>\n",
       "      <td>2.45 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1512638.0</td>\n",
       "      <td>98.46 %</td>\n",
       "      <td>98.46 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_CONV_L</th>\n",
       "      <td>105989.0</td>\n",
       "      <td>105989.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>430556.0</td>\n",
       "      <td>121760.0</td>\n",
       "      <td>1.32 s</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4950366.0</td>\n",
       "      <td>96.15 %</td>\n",
       "      <td>96.92 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_DENS_S_DO</th>\n",
       "      <td>24405.0</td>\n",
       "      <td>24405.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99532.0</td>\n",
       "      <td>26968.0</td>\n",
       "      <td>0.57 s</td>\n",
       "      <td>17.0</td>\n",
       "      <td>48730.0</td>\n",
       "      <td>94.62 %</td>\n",
       "      <td>96.15 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_DENS_M_DO</th>\n",
       "      <td>39705.0</td>\n",
       "      <td>39705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161304.0</td>\n",
       "      <td>43280.0</td>\n",
       "      <td>0.45 s</td>\n",
       "      <td>8.0</td>\n",
       "      <td>79230.0</td>\n",
       "      <td>93.85 %</td>\n",
       "      <td>94.62 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only_DENS_L_DO</th>\n",
       "      <td>56255.0</td>\n",
       "      <td>56255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228076.0</td>\n",
       "      <td>60832.0</td>\n",
       "      <td>0.67 s</td>\n",
       "      <td>13.0</td>\n",
       "      <td>112230.0</td>\n",
       "      <td>95.38 %</td>\n",
       "      <td>95.38 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_1_S_DO</th>\n",
       "      <td>57477.0</td>\n",
       "      <td>57477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234828.0</td>\n",
       "      <td>64888.0</td>\n",
       "      <td>0.76 s</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2221534.0</td>\n",
       "      <td>94.62 %</td>\n",
       "      <td>94.62 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_1_L_DO</th>\n",
       "      <td>228869.0</td>\n",
       "      <td>228869.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>920396.0</td>\n",
       "      <td>237984.0</td>\n",
       "      <td>1.03 s</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8334238.0</td>\n",
       "      <td>96.92 %</td>\n",
       "      <td>96.92 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_2_S_DO</th>\n",
       "      <td>80133.0</td>\n",
       "      <td>80133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325264.0</td>\n",
       "      <td>87328.0</td>\n",
       "      <td>1.31 s</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1618270.0</td>\n",
       "      <td>96.15 %</td>\n",
       "      <td>96.15 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_2_L_DO</th>\n",
       "      <td>319237.0</td>\n",
       "      <td>319237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1281680.0</td>\n",
       "      <td>328136.0</td>\n",
       "      <td>1.84 s</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5374622.0</td>\n",
       "      <td>96.15 %</td>\n",
       "      <td>96.15 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total parameters  Trainable parameters  \\\n",
       "baseline_linear             1195.0                1195.0   \n",
       "only_DENS_S                24405.0               24405.0   \n",
       "only_DENS_M                39705.0               39705.0   \n",
       "only_DENS_L                56255.0               56255.0   \n",
       "CONV_DENS_1_S              57477.0               57477.0   \n",
       "CONV_DENS_1_L             228869.0              228869.0   \n",
       "CONV_DENS_2_S              80133.0               80133.0   \n",
       "CONV_DENS_2_L             319237.0              319237.0   \n",
       "only_CONV_S                26757.0               26757.0   \n",
       "only_CONV_L               105989.0              105989.0   \n",
       "only_DENS_S_DO             24405.0               24405.0   \n",
       "only_DENS_M_DO             39705.0               39705.0   \n",
       "only_DENS_L_DO             56255.0               56255.0   \n",
       "CONV_DENS_1_S_DO           57477.0               57477.0   \n",
       "CONV_DENS_1_L_DO          228869.0              228869.0   \n",
       "CONV_DENS_2_S_DO           80133.0               80133.0   \n",
       "CONV_DENS_2_L_DO          319237.0              319237.0   \n",
       "\n",
       "                  Non-trainable parameters       Size  Optimized size  \\\n",
       "baseline_linear                        0.0     6104.0          2736.0   \n",
       "only_DENS_S                            0.0    99532.0         26968.0   \n",
       "only_DENS_M                            0.0   161304.0         43280.0   \n",
       "only_DENS_L                            0.0   228076.0         60832.0   \n",
       "CONV_DENS_1_S                          0.0   234828.0         64888.0   \n",
       "CONV_DENS_1_L                          0.0   920396.0        237984.0   \n",
       "CONV_DENS_2_S                          0.0   325264.0         87328.0   \n",
       "CONV_DENS_2_L                          0.0  1281680.0        328136.0   \n",
       "only_CONV_S                            0.0   113628.0         38424.0   \n",
       "only_CONV_L                            0.0   430556.0        121760.0   \n",
       "only_DENS_S_DO                         0.0    99532.0         26968.0   \n",
       "only_DENS_M_DO                         0.0   161304.0         43280.0   \n",
       "only_DENS_L_DO                         0.0   228076.0         60832.0   \n",
       "CONV_DENS_1_S_DO                       0.0   234828.0         64888.0   \n",
       "CONV_DENS_1_L_DO                       0.0   920396.0        237984.0   \n",
       "CONV_DENS_2_S_DO                       0.0   325264.0         87328.0   \n",
       "CONV_DENS_2_L_DO                       0.0  1281680.0        328136.0   \n",
       "\n",
       "                 Training time GPU  Epochs      FLOPS Full model accuracy  \\\n",
       "baseline_linear             0.30 s     3.0     2410.0             80.77 %   \n",
       "only_DENS_S                 0.57 s    15.0    48730.0             96.15 %   \n",
       "only_DENS_M                 0.53 s    12.0    79230.0             96.92 %   \n",
       "only_DENS_L                 0.53 s    12.0   112230.0             97.69 %   \n",
       "CONV_DENS_1_S               1.78 s     4.0  2221534.0             93.85 %   \n",
       "CONV_DENS_1_L               1.07 s     5.0  8334238.0             96.92 %   \n",
       "CONV_DENS_2_S               0.66 s     5.0  1618270.0             94.62 %   \n",
       "CONV_DENS_2_L               1.00 s     6.0  5374622.0             96.92 %   \n",
       "only_CONV_S                 2.45 s    12.0  1512638.0             98.46 %   \n",
       "only_CONV_L                 1.32 s     8.0  4950366.0             96.15 %   \n",
       "only_DENS_S_DO              0.57 s    17.0    48730.0             94.62 %   \n",
       "only_DENS_M_DO              0.45 s     8.0    79230.0             93.85 %   \n",
       "only_DENS_L_DO              0.67 s    13.0   112230.0             95.38 %   \n",
       "CONV_DENS_1_S_DO            0.76 s     5.0  2221534.0             94.62 %   \n",
       "CONV_DENS_1_L_DO            1.03 s     5.0  8334238.0             96.92 %   \n",
       "CONV_DENS_2_S_DO            1.31 s    11.0  1618270.0             96.15 %   \n",
       "CONV_DENS_2_L_DO            1.84 s    10.0  5374622.0             96.15 %   \n",
       "\n",
       "                 Optimized model accuracy  \n",
       "baseline_linear                   79.23 %  \n",
       "only_DENS_S                       96.92 %  \n",
       "only_DENS_M                       96.92 %  \n",
       "only_DENS_L                       97.69 %  \n",
       "CONV_DENS_1_S                     95.38 %  \n",
       "CONV_DENS_1_L                     97.69 %  \n",
       "CONV_DENS_2_S                     95.38 %  \n",
       "CONV_DENS_2_L                     96.92 %  \n",
       "only_CONV_S                       98.46 %  \n",
       "only_CONV_L                       96.92 %  \n",
       "only_DENS_S_DO                    96.15 %  \n",
       "only_DENS_M_DO                    94.62 %  \n",
       "only_DENS_L_DO                    95.38 %  \n",
       "CONV_DENS_1_S_DO                  94.62 %  \n",
       "CONV_DENS_1_L_DO                  96.92 %  \n",
       "CONV_DENS_2_S_DO                  96.15 %  \n",
       "CONV_DENS_2_L_DO                  96.15 %  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as tfm\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow.keras.callbacks as tfc\n",
    "import tensorflow.keras.utils as tfu\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import keras_flops as kf\n",
    "import time\n",
    "import functools\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "IMAGE_HEIGHT = 40\n",
    "IMAGE_WIDTH = 40\n",
    "SAMPLES_PER_MEASUREMENT = 119\n",
    "LINES_PER_MEASUREMENT = SAMPLES_PER_MEASUREMENT + 1\n",
    "IMAGE_WIDTH_HEIGHT_INDEX = IMAGE_WIDTH - 1\n",
    "NUMBER_OF_LABELS = 5\n",
    "LABELS = [\"Avada Kedavra\", \"Locomotor\", \"Arresto Momentum\", \"Revelio\", \"Alohomora\"]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# create input/output directories and download data\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "if not os.path.isfile(\"data/spells.zip\"):\n",
    "    os.system(\"wget -P data/ https://github.com/xmihol00/embedded_ML/raw/main/data/spells.zip\")\n",
    "    os.system(\"unzip data/spells.zip\")\n",
    "\n",
    "def representative_dataset(data_set):\n",
    "    for sample in data_set:\n",
    "        yield [np.expand_dims(sample, 0)]\n",
    "\n",
    "def collect_model_summary(summary_line, model_dict):\n",
    "    match = re.match(r\"(.*?): ([\\d,]+)\", summary_line)\n",
    "    if match:\n",
    "        match = match.groups()\n",
    "        model_dict[match[0].replace(\"params\", \"parameters\")] = int(match[1].replace(',', ''))\n",
    "\n",
    "def get_stroke_samples(data):\n",
    "    orientation_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 3))\n",
    "    stroke_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 2))\n",
    "    rows_of_samples = [list(map(lambda x: float(x), line.split(','))) for line in data.split('\\n') if line]\n",
    "\n",
    "    for i in range(0, len(rows_of_samples), SAMPLES_PER_MEASUREMENT): \n",
    "        measurment = np.array(rows_of_samples[i: i+SAMPLES_PER_MEASUREMENT])\n",
    "        acceleration_average = np.average(measurment[:, 0:3], axis=0)\n",
    "\n",
    "        # calcualte orientation\n",
    "        previous_orientation = np.zeros(3)\n",
    "        for j, gyro_sample in enumerate(measurment[:, 3:6]):\n",
    "            orientation_samples[j] = previous_orientation + gyro_sample / SAMPLES_PER_MEASUREMENT\n",
    "            previous_orientation = orientation_samples[j]     \n",
    "        orientation_avg = np.average(orientation_samples, axis=0) # average orientation\n",
    "\n",
    "        # calculate stroke\n",
    "        acceleration_magnitude = np.sqrt(acceleration_average.dot(acceleration_average.T)) # dot product insted of squaring\n",
    "        acceleration_magnitude += (acceleration_magnitude < 0.0001) * 0.0001 # prevent division by 0\n",
    "        normalzied_acceleration = acceleration_average / acceleration_magnitude\n",
    "        normalized_orientation = orientation_samples - orientation_avg\n",
    "        stroke_samples[:, 0] = -normalzied_acceleration[1] * normalized_orientation[:, 1] - normalzied_acceleration[2] * normalized_orientation[:, 2]\n",
    "        stroke_samples[:, 1] =  normalzied_acceleration[1] * normalized_orientation[:, 2] - normalzied_acceleration[2] * normalized_orientation[:, 1]\n",
    "        yield stroke_samples\n",
    "\n",
    "def load_as_images(one_hot=True):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    for i, file_name in enumerate(glob.glob(\"data/*.csv\")):\n",
    "        file = open(file_name, \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    colors = np.linspace(255 - 2 * SAMPLES_PER_MEASUREMENT + 2, 255, SAMPLES_PER_MEASUREMENT) / 255\n",
    "    images = np.zeros((len(labels), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        # rasterize stroke\n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        pixels = np.round(stroke_samples * IMAGE_WIDTH_HEIGHT_INDEX / np.max(stroke_samples, axis=0), 0).astype(np.uint8) # normalize samples to the whole image\n",
    "        image = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image[pixels[:, 1], pixels[:, 0]] = colors\n",
    "        images[i] = image.reshape(IMAGE_WIDTH, IMAGE_HEIGHT, 1).astype(np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = skms.train_test_split(images, labels, test_size=0.2, random_state=SEED)\n",
    "    if one_hot:\n",
    "        # one-hot encoding of labels\n",
    "        y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "\n",
    "    return X_train, X_test, y_train, np.array(y_test)\n",
    "\n",
    "def load_as_array(one_hot=True):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    for i, file_name in enumerate(glob.glob(\"data/*.csv\")):\n",
    "        file = open(file_name, \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    arrays = np.zeros((len(labels), 2 * SAMPLES_PER_MEASUREMENT), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        stroke_samples /= np.max(stroke_samples, axis=0) # normalize values from 0 to 1\n",
    "        arrays[i] = stroke_samples.reshape(-1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = skms.train_test_split(arrays, labels, test_size=0.2, random_state=SEED)\n",
    "    if one_hot:\n",
    "        # one-hot encoding of labels\n",
    "        y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "\n",
    "    return X_train, X_test, y_train, np.array(y_test)\n",
    "\n",
    "\n",
    "hidden_activation = tf.keras.layers.LeakyReLU(0.1)\n",
    "droput_1 = 0.4\n",
    "droput_2 = 0.3\n",
    "droput_3 = 0.25\n",
    "\n",
    "models = [\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=125, activation=hidden_activation),\n",
    "        tfl.Dense(units=75, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=150, activation=hidden_activation),\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dense(units=50, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=128, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=128, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(1, 1), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=128, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(1, 1), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=125, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=75, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=150, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=50, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_3),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=128, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=128, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"baseline_linear\",\n",
    "    \"only_DENS_S\", \"only_DENS_M\", \"only_DENS_L\",\n",
    "    \"CONV_DENS_1_S\", \"CONV_DENS_1_L\", \n",
    "    \"CONV_DENS_2_S\", \"CONV_DENS_2_L\", \n",
    "    \"only_CONV_S\", \"only_CONV_L\",\n",
    "    \"only_DENS_S_DO\", \"only_DENS_M_DO\", \"only_DENS_L_DO\",\n",
    "    \"CONV_DENS_1_S_DO\", \"CONV_DENS_1_L_DO\", \n",
    "    \"CONV_DENS_2_S_DO\", \"CONV_DENS_2_L_DO\"\n",
    "]\n",
    "\n",
    "model_data_set = [\n",
    "    0,\n",
    "    0, 0, 0,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "    0, 0, 0,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "]\n",
    "\n",
    "table_header = [\"Total parameters\", \"Trainable parameters\", \"Non-trainable parameters\", \"Size\", \"Optimized size\", \n",
    "                \"Training time GPU\", \"Epochs\", \"FLOPS\", \"Full model accuracy\", \"Optimized model accuracy\"]\n",
    "\n",
    "data_sets = [load_as_array(), load_as_images()]\n",
    "results = {}\n",
    "\n",
    "for model, model_name, data_set in zip(models, model_names, model_data_set):\n",
    "    X_train, X_test, y_train, y_test = data_sets[data_set]\n",
    "    results[model_name] = {}\n",
    "    results_model = results[model_name]\n",
    "\n",
    "    # get weights for the given seed\n",
    "    model.build(X_train.shape)\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    # get the best number of epochs based on validation data set\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=16, verbose=2,\n",
    "                        callbacks=[tfc.EarlyStopping(monitor=\"val_accuracy\", patience=3, mode=\"max\", restore_best_weights=False)]).history\n",
    "    model.set_weights(weights)\n",
    "    epochs = len(history[\"loss\"]) - 3\n",
    "    results_model[\"Epochs\"] = epochs\n",
    "    \n",
    "    # train on the whole train data set\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    train_start = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.0, batch_size=16, verbose=2)\n",
    "    results_model[\"Training time GPU\"] = f\"{time.time() - train_start:.2f} s\"\n",
    "\n",
    "    # predict and evaluate the prediction\n",
    "    predictions = model.predict(X_test, verbose=2)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    results_model[\"Full model accuracy\"] = f\"{(predictions == y_test).sum() / y_test.shape[0] * 100:.2f} \\\\%\"\n",
    "\n",
    "    # plot the full model confusion metrix\n",
    "    figure, axis = plt.subplots(2, 1, figsize=(12, 18))\n",
    "    figure.suptitle(f\"{model_name} confusion matrices\", fontsize=16) \n",
    "    confusion_matrix = tf.math.confusion_matrix(y_test, predictions).numpy()\n",
    "    confusion_matrix = skm.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=LABELS)\n",
    "    axis[0].set_title(f\"Full model\")\n",
    "    confusion_matrix.plot(cmap=\"Blues\", ax=axis[0])\n",
    "\n",
    "    # get the summary of the model\n",
    "    model.summary(print_fn=lambda x, y=results_model: collect_model_summary(x, y))\n",
    "    results_model[\"FLOPS\"] = kf.get_flops(model, batch_size=1)\n",
    "\n",
    "    # convert the model without optimiziation (evaluation is not necessary, the results after conversion are the same)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    results_file = open(f\"models/{model_name}.tflite\", \"wb\")\n",
    "    results_file.write(tflite_model)\n",
    "    results_file.close()\n",
    "    results_model[\"Size\"] = os.path.getsize(f\"models/{model_name}.tflite\")\n",
    "    os.system(f'echo \"const unsigned char model[] = {{\" > models/{model_name}.h && cat models/{model_name}.tflite | xxd -i >> models/{model_name}.h && echo \"}};\" >> models/{model_name}.h && rm -f models/{model_name}.tflite')\n",
    "    del tflite_model\n",
    "\n",
    "    # convert the model with optimization \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    converter.representative_dataset = lambda x=X_train: representative_dataset(x)\n",
    "    tflite_model_opt = converter.convert()\n",
    "    results_file = open(f\"models/{model_name}.tflite\", \"wb\")\n",
    "    results_file.write(tflite_model_opt)\n",
    "    results_file.close()\n",
    "    results_model[\"Optimized size\"] = os.path.getsize(f\"models/{model_name}.tflite\")\n",
    "    os.system(f'echo \"const unsigned char model[] = {{\" > models/{model_name}_opt.h && cat models/{model_name}.tflite | xxd -i >> models/{model_name}_opt.h && echo \"}};\" >> models/{model_name}_opt.h && rm -f models/{model_name}.tflite')\n",
    "\n",
    "    # predict using the optimized model and evaluate the prediction\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model_opt)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    input_scale, input_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "    predictions = np.zeros((y_test.shape[0]))\n",
    "    for i, sample in enumerate(X_test):\n",
    "        interpreter.set_tensor(input_index, np.expand_dims(sample / input_scale + input_zero_point, 0).astype(np.int8))\n",
    "        interpreter.invoke()\n",
    "        predictions[i] = np.argmax(interpreter.get_tensor(output_index)[0]) # rescaling is not needed\n",
    "    results_model[\"Optimized model accuracy\"] = f\"{(predictions == y_test).sum() / y_test.shape[0] * 100:.2f} \\\\%\"\n",
    "    \n",
    "    # plot the confusuion matrix of the optimized model\n",
    "    confusion_matrix = tf.math.confusion_matrix(y_test, predictions).numpy()\n",
    "    confusion_matrix = skm.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=LABELS)\n",
    "    axis[1].set_title(f\"Optimized model\")\n",
    "    confusion_matrix.plot(cmap=\"Blues\", ax=axis[1])\n",
    "    plt.savefig(f\"figures/{model_name}_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "    del tflite_model_opt\n",
    "\n",
    "    # clear cell output after each model\n",
    "    ipd.clear_output()\n",
    "\n",
    "# export colected statistics to LaTex table and pandas data frame\n",
    "data_frame = pd.DataFrame()\n",
    "with open(\"results/statistics.tex\", \"w\") as results_file:\n",
    "    print = functools.partial(print, file=results_file)\n",
    "    row_end = \"\\\\\\\\\"\n",
    "    backslash_underscore = \"\\\\_\"\n",
    "    print(\"\\\\begin{table}[ht]\", \"\\\\tiny\", \"\\\\center\", \"\\\\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }\", sep=\"\\n\")        \n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    print(\"& \", end=\"\")\n",
    "    for header in table_header[:-1]:\n",
    "        print(f\"\\\\thead{{{header.replace(' ', row_end)}}} & \", end=\"\")\n",
    "    print(f\"\\\\thead{{{table_header[-1].replace(' ', row_end)}}} {row_end}\")\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    for model_name in model_names:\n",
    "        results_model = results[model_name]\n",
    "        print(f\"\\\\thead{{{model_name.replace('_', backslash_underscore)}}} & \", end=\"\")\n",
    "        for header in table_header[:-1]:\n",
    "            print(f\"{results_model[header]} & \", end=\"\")\n",
    "            data_frame.at[model_name, header] = results_model[header]\n",
    "        print(f\"{results_model[table_header[-1]]} {row_end}\")\n",
    "        data_frame.at[model_name, table_header[-1]] = results_model[table_header[-1]]\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\end{tabular}\", \"\\\\end{table}\", sep=\"\\n\")\n",
    "\n",
    "data_frame.replace(\"\\\\\\%\", '%', regex=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
