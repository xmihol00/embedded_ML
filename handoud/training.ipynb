{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the test data set: 93.08 %\n",
      "Total parameters: 24405\n",
      "Trainable parameters: 24405\n",
      "Non-trainable parameters: 0\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=[<tf.Tensor 'args_0:0' shape=(1, 238) dtype=float32>]. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=[<tf.Tensor 'args_0:0' shape=(1, 238) dtype=float32>]. Consider rewriting this model with the Functional API.\n",
      "2022-12-31 19:27:07.612361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 19:27:07.612646: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2022-12-31 19:27:07.612707: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-12-31 19:27:07.613015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 19:27:07.613253: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING:absl:Found untraced functions such as leaky_re_lu_2_layer_call_fn, leaky_re_lu_2_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "ProfiFLOPS: 48730\n",
      "le:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/48.73k flops)\n",
      "  sequential_4/dense_4/MatMul (47.60k/47.60k flops)\n",
      "  sequential_4/dense_5/MatMul (1.00k/1.00k flops)\n",
      "  sequential_4/dense_4/BiasAdd (100/100 flops)\n",
      "  sequential_4/dense_5/Softmax (25/25 flops)\n",
      "  sequential_4/dense_5/BiasAdd (5/5 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp42szbaym/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp42szbaym/assets\n",
      "2022-12-31 19:27:08.008492: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-12-31 19:27:08.008516: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-12-31 19:27:08.008628: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp42szbaym\n",
      "2022-12-31 19:27:08.009231: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-12-31 19:27:08.009252: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp42szbaym\n",
      "2022-12-31 19:27:08.011436: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-12-31 19:27:08.034212: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp42szbaym\n",
      "2022-12-31 19:27:08.039608: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 30980 microseconds.\n",
      "WARNING:absl:Found untraced functions such as leaky_re_lu_2_layer_call_fn, leaky_re_lu_2_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 99488\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpt94ahz75/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt94ahz75/assets\n",
      "/home/david/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized model size: 26928\n",
      "Optimaized model accuracy on the test data set: 93.08 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 19:27:08.432008: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-12-31 19:27:08.432032: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-12-31 19:27:08.432145: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpt94ahz75\n",
      "2022-12-31 19:27:08.432729: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-12-31 19:27:08.432745: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpt94ahz75\n",
      "2022-12-31 19:27:08.434704: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-12-31 19:27:08.457240: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpt94ahz75\n",
      "2022-12-31 19:27:08.462832: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 30686 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.utils as tfu\n",
    "import tensorflow.keras.models as tfm\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow.keras.callbacks as tfc\n",
    "import tensorflow.keras.regularizers as tfr\n",
    "import tensorflow.keras.initializers as tfi\n",
    "import sklearn.model_selection as skm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import keras_flops as kf\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "IMAGE_HEIGHT = 40\n",
    "IMAGE_WIDTH = 40\n",
    "SAMPLES_PER_MEASUREMENT = 119\n",
    "LINES_PER_MEASUREMENT = SAMPLES_PER_MEASUREMENT + 1\n",
    "IMAGE_WIDTH_HEIGHT_INDEX = IMAGE_WIDTH - 1\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "def representative_dataset(data_set):\n",
    "    for sample in data_set:\n",
    "        yield [np.expand_dims(sample, 0)]\n",
    "\n",
    "def collect_model_summary(summary_line, model_dict):\n",
    "    match = re.match(r\"(.*?): ([\\d,]+)\", summary_line)\n",
    "    if match:\n",
    "        match = match.groups()\n",
    "        model_dict[match[0].replace(\"params\", \"parameters\")] = int(match[1].replace(',', ''))\n",
    "\n",
    "def get_stroke_samples(data):\n",
    "    orientation_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 3))\n",
    "    stroke_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 2))\n",
    "    rows_of_samples = [list(map(lambda x: float(x), line.split(','))) for line in data.split('\\n') if line]\n",
    "\n",
    "    for i in range(0, len(rows_of_samples), SAMPLES_PER_MEASUREMENT): \n",
    "        measurment = np.array(rows_of_samples[i: i+SAMPLES_PER_MEASUREMENT])\n",
    "        acceleration_average = np.average(measurment[:, 0:3], axis=0)\n",
    "\n",
    "        # calcualte orientation\n",
    "        previous_orientation = np.zeros(3)\n",
    "        for j, gyro_sample in enumerate(measurment[:, 3:6]):\n",
    "            orientation_samples[j] = previous_orientation + gyro_sample / SAMPLES_PER_MEASUREMENT\n",
    "            previous_orientation = orientation_samples[j]     \n",
    "        orientation_avg = np.average(orientation_samples, axis=0) # average orientation\n",
    "\n",
    "        # calculate stroke\n",
    "        acceleration_magnitude = np.sqrt(acceleration_average.dot(acceleration_average.T)) # dot product insted of squaring\n",
    "        acceleration_magnitude += (acceleration_magnitude < 0.0001) * 0.0001 # prevent division by 0\n",
    "        normalzied_acceleration = acceleration_average / acceleration_magnitude\n",
    "        normalized_orientation = orientation_samples - orientation_avg\n",
    "        stroke_samples[:, 0] = -normalzied_acceleration[1] * normalized_orientation[:, 1] - normalzied_acceleration[2] * normalized_orientation[:, 2]\n",
    "        stroke_samples[:, 1] =  normalzied_acceleration[1] * normalized_orientation[:, 2] - normalzied_acceleration[2] * normalized_orientation[:, 1]\n",
    "        yield stroke_samples\n",
    "\n",
    "def load_as_images(one_hot=True):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    for i, file_name in enumerate(os.listdir(\"../data\")):\n",
    "        file = open(f\"../data/{file_name}\", \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    colors = np.linspace(255 - 2 * SAMPLES_PER_MEASUREMENT + 2, 255, SAMPLES_PER_MEASUREMENT) / 255\n",
    "    images = np.zeros((len(labels), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        # rasterize stroke\n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        pixels = np.round(stroke_samples * IMAGE_WIDTH_HEIGHT_INDEX / np.max(stroke_samples, axis=0), 0).astype(np.uint8) # normalize samples to the whole image\n",
    "        image = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image[pixels[:, 1], pixels[:, 0]] = colors\n",
    "        images[i] = image.reshape(IMAGE_WIDTH, IMAGE_HEIGHT, 1).astype(np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = skm.train_test_split(images, labels, test_size=0.2, random_state=SEED)\n",
    "    if one_hot:\n",
    "        # one-hot encoding of labels\n",
    "        y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "        y_test = tfu.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def load_as_array(one_hot=True):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    for i, file_name in enumerate(os.listdir(\"../data\")):\n",
    "        file = open(f\"../data/{file_name}\", \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    arrays = np.zeros((len(labels), 2 * SAMPLES_PER_MEASUREMENT), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        stroke_samples /= np.max(stroke_samples, axis=0) # normalize values from 0 to 1\n",
    "        arrays[i] = stroke_samples.reshape(-1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = skm.train_test_split(arrays, labels, test_size=0.2, random_state=SEED)\n",
    "    if one_hot:\n",
    "        # one-hot encoding of labels\n",
    "        y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "        y_test = tfu.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "hidden_activation = tf.keras.layers.LeakyReLU(0.1)\n",
    "dense_model = tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "])\n",
    "conv_model = tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(1, 1), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_as_array()\n",
    "\n",
    "# get weights for the given seed\n",
    "dense_model.build(X_train.shape)\n",
    "weights = dense_model.get_weights()\n",
    "\n",
    "# get the best number of epochs based on validation data set\n",
    "dense_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = dense_model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=16, verbose=0,\n",
    "                          callbacks=[tfc.EarlyStopping(monitor=\"val_accuracy\", patience=3, mode=\"max\", restore_best_weights=False)]).history\n",
    "dense_model.set_weights(weights)\n",
    "epochs = len(history[\"loss\"]) - 3\n",
    "\n",
    "# train on the whole train data set\n",
    "dense_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "train_start = time.time()\n",
    "dense_model.fit(X_train, y_train, epochs=epochs, validation_split=0.0, batch_size=16, verbose=0)\n",
    "train_time = f\"{time.time() - train_start:.2f} s\"\n",
    "\n",
    "# evaluate the results\n",
    "print(f\"Model accuracy on the test data set: {dense_model.evaluate(X_test, y_test, verbose=0)[1] * 100:.2f} %\")\n",
    "\n",
    "# get the summary of the model\n",
    "params = {}\n",
    "dense_model.summary(print_fn=lambda x, y=params: collect_model_summary(x, y))\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"FLOPS: {kf.get_flops(dense_model, batch_size=1)}\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(dense_model)\n",
    "tflite_model = converter.convert()\n",
    "results_file = open(f\"dense_model.tflite\", \"wb\")\n",
    "results_file.write(tflite_model)\n",
    "results_file.close()\n",
    "print(f\"Model size: {os.path.getsize('dense_model.tflite')}\")\n",
    "os.system(f\"rm -f dense_model.tflite\")\n",
    "del tflite_model\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(dense_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "converter.representative_dataset = lambda x=X_train: representative_dataset(x)\n",
    "tflite_model_opt = converter.convert()\n",
    "results_file = open(f\"dense_model.tflite\", \"wb\")\n",
    "results_file.write(tflite_model_opt)\n",
    "results_file.close()\n",
    "print(f\"Optimized model size: {os.path.getsize(f'dense_model.tflite')}\")\n",
    "os.system(f'echo \"const unsigned char model[] = {{\" > dense_inference/model.h && cat dense_model.tflite | xxd -i >> dense_inference/model.h && echo \"}};\" >> dense_inference/model.h && rm -f dense_model.tflite')\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_opt)\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "input_scale, input_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "accuracy = 0\n",
    "for i, sample in enumerate(X_test):\n",
    "    interpreter.set_tensor(input_index, np.expand_dims(sample / input_scale + input_zero_point, 0).astype(np.int8))\n",
    "    interpreter.invoke()\n",
    "    accuracy += np.argmax(y_test[i]) == np.argmax(interpreter.get_tensor(output_index)[0]) # rescaling is not needed\n",
    "print(f\"Optimaized model accuracy on the test data set: {accuracy / X_test.shape[0] * 100:.2f} %\")\n",
    "del tflite_model_opt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
