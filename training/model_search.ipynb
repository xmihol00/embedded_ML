{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - imports, constants, download of data from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras-flops in /mnt/sdc3/david/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: tensorflow<3.0,>=2.2 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from keras-flops) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.23.1)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.0.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (4.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.10.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.12.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.14.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (59.6.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (22.9.24)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.50.0)\n",
      "Requirement already satisfied: packaging in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (21.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (14.0.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (2.13.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (3.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->tensorflow<3.0,>=2.2->keras-flops) (2.4.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /mnt/sdc3/david/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as tfm\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow.keras.callbacks as tfc\n",
    "import tensorflow.keras.utils as tfu\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import functools\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "\n",
    "os.system(\"pip install keras-flops\")\n",
    "import keras_flops as kf\n",
    "\n",
    "SEED = 42\n",
    "IMAGE_HEIGHT = 20\n",
    "IMAGE_WIDTH = 20\n",
    "SAMPLES_PER_MEASUREMENT = 119\n",
    "LINES_PER_MEASUREMENT = SAMPLES_PER_MEASUREMENT + 1\n",
    "IMAGE_WIDTH_HEIGHT_INDEX = IMAGE_WIDTH - 1\n",
    "SAMPLES_PER_MEASUREMENT_CROPPED = 110\n",
    "FRONT_CROP = 5\n",
    "BACK_CROP = 4\n",
    "NUMBER_OF_LABELS = 5\n",
    "LABELS = [\"Avada Kedavra\", \"Locomotor\", \"Arresto Momentum\", \"Revelio\", \"Alohomora\"]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "LARGE_DATA_SET = False  # set to True to train on larger data set\n",
    "POOR_MODELS = False     # set to True to train also model, for which training fails\n",
    "FULL_INPUT = False      # set to True to train on full 119 samples of a spell, otherwise start and end are scopped to 110 samples\n",
    "\n",
    "# create input/output directories and download data\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"data_large\", exist_ok=True)\n",
    "\n",
    "if not os.path.isfile(\"data/spells.zip\") and not LARGE_DATA_SET:\n",
    "    os.system(\"wget -P data/ https://github.com/xmihol00/robust_magic_wand/raw/main/data/spells.zip\")\n",
    "    os.system(\"unzip data/spells.zip\")\n",
    "\n",
    "if not os.path.isfile(\"data_large/spells.zip\") and LARGE_DATA_SET:\n",
    "    os.system(\"wget -P data_large/ https://github.com/xmihol00/robust_magic_wand/raw/main/data_large/spells.zip\")\n",
    "    os.system(\"unzip data_large/spells.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing, creation of data sets and model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stroke_samples(data):\n",
    "    angle_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 3))\n",
    "    stroke_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 2))\n",
    "    rows_of_samples = [list(map(lambda x: float(x), line.split(','))) for line in data.split('\\n') if line]\n",
    "\n",
    "    for i in range(0, len(rows_of_samples), SAMPLES_PER_MEASUREMENT): \n",
    "        measurment = np.array(rows_of_samples[i: i+SAMPLES_PER_MEASUREMENT])\n",
    "        acceleration_average = np.average(measurment[:, 0:3], axis=0)\n",
    "\n",
    "        # calcualte angle\n",
    "        previous_angle = np.zeros(3)\n",
    "        for j, gyro_sample in enumerate(measurment[:, 3:6]):\n",
    "            angle_samples[j] = previous_angle + gyro_sample / SAMPLES_PER_MEASUREMENT\n",
    "            previous_angle = angle_samples[j]     \n",
    "        angle_avg = np.average(angle_samples, axis=0) # average angle\n",
    "\n",
    "        # calculate stroke\n",
    "        acceleration_magnitude = np.sqrt(acceleration_average.dot(acceleration_average.T)) # dot product insted of squaring\n",
    "        acceleration_magnitude += (acceleration_magnitude < 0.0001) * 0.0001 # prevent division by 0\n",
    "        normalzied_acceleration = acceleration_average / acceleration_magnitude\n",
    "        normalized_angle = angle_samples - angle_avg\n",
    "        stroke_samples[:, 0] = -normalzied_acceleration[1] * normalized_angle[:, 1] - normalzied_acceleration[2] * normalized_angle[:, 2]\n",
    "        stroke_samples[:, 1] =  normalzied_acceleration[1] * normalized_angle[:, 2] - normalzied_acceleration[2] * normalized_angle[:, 1]\n",
    "        yield stroke_samples\n",
    "\n",
    "FILE_TO_LABLE_DICT = { \"avada_kedavra.csv\" : \"Avada Kedavra\", \"alohomora_charm.csv\" : \"Alohomora\", \"locomotor_charm.csv\" : \"Locomotor\",\n",
    "                       \"revelio.csv\" : \"Revelio\", \"aresto_momentum_charm.csv\" : \"Aresto Momentum\" }\n",
    "\n",
    "def laod_as_arrays_and_images(directory, one_hot=True, seed=SEED):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    print(\"Order of labels:\")\n",
    "    for i, file_name in enumerate(sorted(glob.glob(f\"{directory}/*.csv\"))):\n",
    "        print(' ' * 3, FILE_TO_LABLE_DICT[file_name.replace(f\"{directory}/\", \"\")])\n",
    "        file = open(file_name, \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    arrays = np.zeros((len(labels), 2 * SAMPLES_PER_MEASUREMENT), dtype=np.float32)\n",
    "    colors = np.linspace(255 - 2 * SAMPLES_PER_MEASUREMENT + 2, 255, SAMPLES_PER_MEASUREMENT) / 255\n",
    "    images = np.zeros((len(labels), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        stroke_samples /= np.max(stroke_samples, axis=0) # normalize values from 0 to 1\n",
    "        arrays[i] = stroke_samples.reshape(-1)\n",
    "        \n",
    "        # rasterize stroke\n",
    "        pixels = np.round(stroke_samples * IMAGE_WIDTH_HEIGHT_INDEX, 0).astype(np.uint8) # normalize samples to the whole image\n",
    "        image = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image[pixels[:, 1], pixels[:, 0]] = colors\n",
    "        images[i] = image.reshape(IMAGE_WIDTH, IMAGE_HEIGHT, 1).astype(np.float32)\n",
    "\n",
    "\n",
    "    both_variants = [skms.train_test_split(arrays, labels, test_size=0.2, random_state=seed),\n",
    "                     skms.train_test_split(images, labels, test_size=0.2, random_state=seed)]\n",
    "\n",
    "    for i in range(len(both_variants)):\n",
    "        X_train, X_test, y_train, y_test = both_variants[i]\n",
    "        y_test = np.array(y_test)\n",
    "        if one_hot:\n",
    "            # one-hot encoding of labels\n",
    "            y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "        both_variants[i] = (X_train, X_test, y_train, y_test)\n",
    "       \n",
    "    return both_variants\n",
    "\n",
    "def laod_as_arrays_and_images_cropped(directory, one_hot=True, seed=SEED):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    print(\"Order of labels:\")\n",
    "    for i, file_name in enumerate(sorted(glob.glob(f\"{directory}/*.csv\"))):\n",
    "        print(' ' * 3, FILE_TO_LABLE_DICT[file_name.replace(f\"{directory}/\", \"\")])\n",
    "        file = open(file_name, \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    arrays = np.zeros((len(labels), 2 * SAMPLES_PER_MEASUREMENT_CROPPED), dtype=np.float32)\n",
    "    colors = np.linspace(255 - 2 * SAMPLES_PER_MEASUREMENT_CROPPED + 2, 255, SAMPLES_PER_MEASUREMENT_CROPPED) / 255\n",
    "    images = np.zeros((len(labels), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        stroke_samples /= np.max(stroke_samples, axis=0) # normalize values from 0 to 1\n",
    "        stroke_samples = stroke_samples[FRONT_CROP:-BACK_CROP, :]\n",
    "        arrays[i] = stroke_samples.reshape(-1)\n",
    "        \n",
    "        # rasterize stroke\n",
    "        pixels = np.round(stroke_samples * IMAGE_WIDTH_HEIGHT_INDEX, 0).astype(np.uint8) # normalize samples to the whole image\n",
    "        image = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image[pixels[:, 1], pixels[:, 0]] = colors\n",
    "        images[i] = image.reshape(IMAGE_WIDTH, IMAGE_HEIGHT, 1).astype(np.float32)\n",
    "\n",
    "\n",
    "    both_variants = [skms.train_test_split(arrays, labels, test_size=0.2, random_state=seed),\n",
    "                     skms.train_test_split(images, labels, test_size=0.2, random_state=seed)]\n",
    "\n",
    "    for i in range(len(both_variants)):\n",
    "        X_train, X_test, y_train, y_test = both_variants[i]\n",
    "        y_test = np.array(y_test)\n",
    "        if one_hot:\n",
    "            # one-hot encoding of labels\n",
    "            y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "        both_variants[i] = (X_train, X_test, y_train, y_test)\n",
    "       \n",
    "    return both_variants\n",
    "\n",
    "def representative_dataset(data_set):\n",
    "    for sample in data_set:\n",
    "        yield [np.expand_dims(sample, 0)]\n",
    "\n",
    "def collect_model_summary(summary_line, model_dict):\n",
    "    match = re.match(r\"(.*?): ([\\d,]+)\", summary_line)\n",
    "    if match:\n",
    "        match = match.groups()\n",
    "        model_dict[match[0].replace(\"params\", \"parameters\")] = int(match[1].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_activation = tf.keras.layers.LeakyReLU(0.1)\n",
    "droput_1 = 0.4\n",
    "droput_2 = 0.3\n",
    "droput_3 = 0.25\n",
    "\n",
    "models = [\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=125, activation=hidden_activation),\n",
    "        tfl.Dense(units=75, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=32, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=32, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=128, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=125, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=75, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=32, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=32, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=128, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=32, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"baseline_linear\",\n",
    "    \"Only_DENS_S\", \"Only_DENS_L\",\n",
    "    \"CONV_DENS_S\", \"CONV_DENS_L\", \n",
    "    \"Only_CONV_S\", \"Only_CONV_L\",\n",
    "    \"Only_DENS_DO_S\", \"Only_DENS_DO_L\",\n",
    "    \"CONV_DENS_DO_S\", \"CONV_DENS_DO_L\",\n",
    "    \"Only_CONV_BN_S\", \"Only_CONV_BN_L\",\n",
    "    \"CONV_DENS_BN_DO_S\", \"CONV_DENS_BN_DO_L\",\n",
    "]\n",
    "\n",
    "model_data_sets = [\n",
    "    0,\n",
    "    0, 0,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "    0, 0,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "    1, 1\n",
    "]\n",
    "\n",
    "selected_models = [ \"Only_DENS_L\", \"CONV_DENS_DO_L\" ]\n",
    "\n",
    "if not POOR_MODELS:\n",
    "    models = models[:-4]\n",
    "    model_names = model_names[:-4]\n",
    "    model_data_sets = model_data_sets[:-4]\n",
    "\n",
    "table_header = [\"Total parameters\", \"Trainable parameters\", \"Non-trainable parameters\", \"Size\", \"Optimized size\", \"Training time GPU\", \n",
    "                \"Epochs\", \"FLOPS\", \"Full model accuracy\", \"Optimized model accuracy\", \"Inference time\", \"Inference time optimized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data sets and get the order of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of labels:\n",
      "    Alohomora\n",
      "    Aresto Momentum\n",
      "    Avada Kedavra\n",
      "    Locomotor\n",
      "    Revelio\n"
     ]
    }
   ],
   "source": [
    "if FULL_INPUT:\n",
    "    data_sets = laod_as_arrays_and_images(\"data_large\" if LARGE_DATA_SET else \"data\")\n",
    "else:\n",
    "    data_sets = laod_as_arrays_and_images_cropped(\"data_large\" if LARGE_DATA_SET else \"data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and collection of statistics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "figure, axis = plt.subplots(2, 2, figsize=(12, 11)) # plot of the confusion matrix for selected models\n",
    "axis_x_idx, axis_y_idx = 0, 0\n",
    "figure.suptitle(f\"Confusion matrices of choosen models\", fontsize=16)\n",
    "plt.subplots_adjust(left=0.055, bottom=0.03, right=0.985, top=0.95, hspace=0.15, wspace=0.15)\n",
    "\n",
    "for model, model_name, data_set in zip(models, model_names, model_data_sets):\n",
    "    X_train, X_test, y_train, y_test = data_sets[data_set]\n",
    "    results[model_name] = {}\n",
    "    results_model = results[model_name]\n",
    "    results_model[\"Inference time\"] = \"-----\"\n",
    "    results_model[\"Inference time optimized\"] = \"-----\"\n",
    "\n",
    "    # get weights for the given seed\n",
    "    model.build(X_train.shape)\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    # get the best number of epochs based on validation data set\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=2,\n",
    "                        callbacks=[tfc.EarlyStopping(monitor=\"val_accuracy\", patience=3, mode=\"max\", restore_best_weights=False)]).history\n",
    "    model.set_weights(weights)\n",
    "    epochs = len(history[\"loss\"]) - 3\n",
    "    results_model[\"Epochs\"] = epochs\n",
    "    \n",
    "    # train on the whole train data set\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    train_start = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.0, batch_size=32, verbose=2)\n",
    "    results_model[\"Training time GPU\"] = f\"{(time.time() - train_start):.2f} s\"\n",
    "\n",
    "    # predict and evaluate the prediction\n",
    "    predictions = model.predict(X_test, verbose=2)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    results_model[\"Full model accuracy\"] = f\"{((predictions == y_test).sum() / y_test.shape[0] * 100):.2f} \\\\%\"\n",
    "\n",
    "    # plot the full model confusion metrix\n",
    "    if model_name in selected_models:\n",
    "        confusion_matrix = tf.math.confusion_matrix(y_test, predictions).numpy()\n",
    "        confusion_matrix = skm.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=LABELS)\n",
    "        axis[axis_x_idx, axis_y_idx].set_title(f\"{model_name} unoptimized\")\n",
    "        confusion_matrix.plot(cmap=\"Blues\", ax=axis[axis_x_idx, axis_y_idx])\n",
    "        axis_y_idx += 1\n",
    "\n",
    "    # get the summary of the model\n",
    "    model.summary(print_fn=lambda x, y=results_model: collect_model_summary(x, y))\n",
    "    results_model[\"FLOPS\"] = kf.get_flops(model, batch_size=1)\n",
    "\n",
    "    # convert the model without optimiziation (evaluation is not necessary, the results after conversion are the same)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    results_file = open(f\"models/{model_name}.tflite\", \"wb\")\n",
    "    results_file.write(tflite_model)\n",
    "    results_file.close()\n",
    "    results_model[\"Size\"] = os.path.getsize(f\"models/{model_name}.tflite\")\n",
    "    os.system(f'echo \"const unsigned char model[] = {{\" > models/{model_name}.h && cat models/{model_name}.tflite | xxd -i >> models/{model_name}.h && echo \"}};\" >> models/{model_name}.h && rm -f models/{model_name}.tflite')\n",
    "    del tflite_model\n",
    "\n",
    "    # convert the model with optimization \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    converter.representative_dataset = lambda x=X_train: representative_dataset(x)\n",
    "    tflite_model_opt = converter.convert()\n",
    "    results_file = open(f\"models/{model_name}.tflite\", \"wb\")\n",
    "    results_file.write(tflite_model_opt)\n",
    "    results_file.close()\n",
    "    results_model[\"Optimized size\"] = os.path.getsize(f\"models/{model_name}.tflite\")\n",
    "    os.system(f'echo \"const unsigned char model[] = {{\" > models/{model_name}_opt.h && cat models/{model_name}.tflite | xxd -i >> models/{model_name}_opt.h && echo \"}};\" >> models/{model_name}_opt.h && rm -f models/{model_name}.tflite')\n",
    "\n",
    "    # predict using the optimized model and evaluate the prediction\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model_opt)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    input_scale, input_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "    predictions = np.zeros((y_test.shape[0]))\n",
    "    for i, sample in enumerate(X_test):\n",
    "        interpreter.set_tensor(input_index, np.expand_dims(sample / input_scale + input_zero_point, 0).astype(np.int8))\n",
    "        interpreter.invoke()\n",
    "        predictions[i] = np.argmax(interpreter.get_tensor(output_index)[0]) # rescaling is not needed\n",
    "    results_model[\"Optimized model accuracy\"] = f\"{((predictions == y_test).sum() / y_test.shape[0] * 100):.2f} \\\\%\"\n",
    "    \n",
    "    # plot the confusuion matrix of the optimized model\n",
    "    if model_name in selected_models:\n",
    "        confusion_matrix = tf.math.confusion_matrix(y_test, predictions).numpy()\n",
    "        confusion_matrix = skm.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=LABELS)\n",
    "        axis[axis_x_idx, axis_y_idx].set_title(f\"{model_name} optimized\")\n",
    "        confusion_matrix.plot(cmap=\"Blues\", ax=axis[axis_x_idx, axis_y_idx])\n",
    "        axis_y_idx += 1\n",
    "    \n",
    "    del tflite_model_opt\n",
    "\n",
    "    # clear cell output after each model\n",
    "    ipd.clear_output()\n",
    "\n",
    "plt.savefig(f\"figures/confusion_matrix.png\", dpi=300)\n",
    "plt.plot()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total parameters</th>\n",
       "      <th>Trainable parameters</th>\n",
       "      <th>Non-trainable parameters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Optimized size</th>\n",
       "      <th>Training time GPU</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>FLOPS</th>\n",
       "      <th>Full model accuracy</th>\n",
       "      <th>Optimized model accuracy</th>\n",
       "      <th>Inference time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_linear</th>\n",
       "      <td>1105.0</td>\n",
       "      <td>1105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5740.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>0.57 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>86.00 %</td>\n",
       "      <td>85.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_S</th>\n",
       "      <td>22605.0</td>\n",
       "      <td>22605.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92320.0</td>\n",
       "      <td>25168.0</td>\n",
       "      <td>0.39 s</td>\n",
       "      <td>6.0</td>\n",
       "      <td>45130.0</td>\n",
       "      <td>89.00 %</td>\n",
       "      <td>87.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_L</th>\n",
       "      <td>37455.0</td>\n",
       "      <td>37455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152284.0</td>\n",
       "      <td>41024.0</td>\n",
       "      <td>0.49 s</td>\n",
       "      <td>9.0</td>\n",
       "      <td>74730.0</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_S</th>\n",
       "      <td>10181.0</td>\n",
       "      <td>10181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45564.0</td>\n",
       "      <td>17472.0</td>\n",
       "      <td>2.47 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>537886.0</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_L</th>\n",
       "      <td>40069.0</td>\n",
       "      <td>40069.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165116.0</td>\n",
       "      <td>48968.0</td>\n",
       "      <td>1.52 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2013726.0</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_S</th>\n",
       "      <td>23877.0</td>\n",
       "      <td>23877.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100432.0</td>\n",
       "      <td>32584.0</td>\n",
       "      <td>1.69 s</td>\n",
       "      <td>7.0</td>\n",
       "      <td>584670.0</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_L</th>\n",
       "      <td>93829.0</td>\n",
       "      <td>93829.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380240.0</td>\n",
       "      <td>105560.0</td>\n",
       "      <td>1.03 s</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1906590.0</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>97.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_DO_S</th>\n",
       "      <td>22605.0</td>\n",
       "      <td>22605.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92320.0</td>\n",
       "      <td>25168.0</td>\n",
       "      <td>0.49 s</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45130.0</td>\n",
       "      <td>91.00 %</td>\n",
       "      <td>91.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_DO_L</th>\n",
       "      <td>37455.0</td>\n",
       "      <td>37455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152284.0</td>\n",
       "      <td>41024.0</td>\n",
       "      <td>0.64 s</td>\n",
       "      <td>7.0</td>\n",
       "      <td>74730.0</td>\n",
       "      <td>92.00 %</td>\n",
       "      <td>91.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_DO_S</th>\n",
       "      <td>10181.0</td>\n",
       "      <td>10181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45564.0</td>\n",
       "      <td>17472.0</td>\n",
       "      <td>1.29 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>537886.0</td>\n",
       "      <td>98.00 %</td>\n",
       "      <td>98.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_DO_L</th>\n",
       "      <td>40069.0</td>\n",
       "      <td>40069.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165116.0</td>\n",
       "      <td>48968.0</td>\n",
       "      <td>1.10 s</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2013726.0</td>\n",
       "      <td>99.00 %</td>\n",
       "      <td>97.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_BN_S</th>\n",
       "      <td>24325.0</td>\n",
       "      <td>24101.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>103224.0</td>\n",
       "      <td>35136.0</td>\n",
       "      <td>0.66 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>595966.0</td>\n",
       "      <td>60.00 %</td>\n",
       "      <td>57.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_BN_L</th>\n",
       "      <td>94725.0</td>\n",
       "      <td>94277.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>383928.0</td>\n",
       "      <td>108336.0</td>\n",
       "      <td>1.24 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1929182.0</td>\n",
       "      <td>62.00 %</td>\n",
       "      <td>64.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_BN_DO_S</th>\n",
       "      <td>10405.0</td>\n",
       "      <td>10293.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>47912.0</td>\n",
       "      <td>19920.0</td>\n",
       "      <td>1.17 s</td>\n",
       "      <td>3.0</td>\n",
       "      <td>549422.0</td>\n",
       "      <td>57.00 %</td>\n",
       "      <td>56.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_BN_DO_L</th>\n",
       "      <td>40517.0</td>\n",
       "      <td>40293.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>167912.0</td>\n",
       "      <td>51528.0</td>\n",
       "      <td>0.69 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2036798.0</td>\n",
       "      <td>51.00 %</td>\n",
       "      <td>46.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Total parameters  Trainable parameters  \\\n",
       "baseline_linear              1105.0                1105.0   \n",
       "Only_DENS_S                 22605.0               22605.0   \n",
       "Only_DENS_L                 37455.0               37455.0   \n",
       "CONV_DENS_S                 10181.0               10181.0   \n",
       "CONV_DENS_L                 40069.0               40069.0   \n",
       "Only_CONV_S                 23877.0               23877.0   \n",
       "Only_CONV_L                 93829.0               93829.0   \n",
       "Only_DENS_DO_S              22605.0               22605.0   \n",
       "Only_DENS_DO_L              37455.0               37455.0   \n",
       "CONV_DENS_DO_S              10181.0               10181.0   \n",
       "CONV_DENS_DO_L              40069.0               40069.0   \n",
       "Only_CONV_BN_S              24325.0               24101.0   \n",
       "Only_CONV_BN_L              94725.0               94277.0   \n",
       "CONV_DENS_BN_DO_S           10405.0               10293.0   \n",
       "CONV_DENS_BN_DO_L           40517.0               40293.0   \n",
       "\n",
       "                   Non-trainable parameters      Size  Optimized size  \\\n",
       "baseline_linear                         0.0    5740.0          2640.0   \n",
       "Only_DENS_S                             0.0   92320.0         25168.0   \n",
       "Only_DENS_L                             0.0  152284.0         41024.0   \n",
       "CONV_DENS_S                             0.0   45564.0         17472.0   \n",
       "CONV_DENS_L                             0.0  165116.0         48968.0   \n",
       "Only_CONV_S                             0.0  100432.0         32584.0   \n",
       "Only_CONV_L                             0.0  380240.0        105560.0   \n",
       "Only_DENS_DO_S                          0.0   92320.0         25168.0   \n",
       "Only_DENS_DO_L                          0.0  152284.0         41024.0   \n",
       "CONV_DENS_DO_S                          0.0   45564.0         17472.0   \n",
       "CONV_DENS_DO_L                          0.0  165116.0         48968.0   \n",
       "Only_CONV_BN_S                        224.0  103224.0         35136.0   \n",
       "Only_CONV_BN_L                        448.0  383928.0        108336.0   \n",
       "CONV_DENS_BN_DO_S                     112.0   47912.0         19920.0   \n",
       "CONV_DENS_BN_DO_L                     224.0  167912.0         51528.0   \n",
       "\n",
       "                  Training time GPU  Epochs      FLOPS Full model accuracy  \\\n",
       "baseline_linear              0.57 s    12.0     2230.0             86.00 %   \n",
       "Only_DENS_S                  0.39 s     6.0    45130.0             89.00 %   \n",
       "Only_DENS_L                  0.49 s     9.0    74730.0             94.00 %   \n",
       "CONV_DENS_S                  2.47 s    12.0   537886.0             95.00 %   \n",
       "CONV_DENS_L                  1.52 s    12.0  2013726.0            100.00 %   \n",
       "Only_CONV_S                  1.69 s     7.0   584670.0             94.00 %   \n",
       "Only_CONV_L                  1.03 s     7.0  1906590.0             95.00 %   \n",
       "Only_DENS_DO_S               0.49 s    10.0    45130.0             91.00 %   \n",
       "Only_DENS_DO_L               0.64 s     7.0    74730.0             92.00 %   \n",
       "CONV_DENS_DO_S               1.29 s    12.0   537886.0             98.00 %   \n",
       "CONV_DENS_DO_L               1.10 s     7.0  2013726.0             99.00 %   \n",
       "Only_CONV_BN_S               0.66 s     1.0   595966.0             60.00 %   \n",
       "Only_CONV_BN_L               1.24 s     1.0  1929182.0             62.00 %   \n",
       "CONV_DENS_BN_DO_S            1.17 s     3.0   549422.0             57.00 %   \n",
       "CONV_DENS_BN_DO_L            0.69 s     1.0  2036798.0             51.00 %   \n",
       "\n",
       "                  Optimized model accuracy Inference time  \n",
       "baseline_linear                    85.00 %          -----  \n",
       "Only_DENS_S                        87.00 %          -----  \n",
       "Only_DENS_L                        94.00 %          -----  \n",
       "CONV_DENS_S                        95.00 %          -----  \n",
       "CONV_DENS_L                       100.00 %          -----  \n",
       "Only_CONV_S                        94.00 %          -----  \n",
       "Only_CONV_L                        97.00 %          -----  \n",
       "Only_DENS_DO_S                     91.00 %          -----  \n",
       "Only_DENS_DO_L                     91.00 %          -----  \n",
       "CONV_DENS_DO_S                     98.00 %          -----  \n",
       "CONV_DENS_DO_L                     97.00 %          -----  \n",
       "Only_CONV_BN_S                     57.00 %          -----  \n",
       "Only_CONV_BN_L                     64.00 %          -----  \n",
       "CONV_DENS_BN_DO_S                  56.00 %          -----  \n",
       "CONV_DENS_BN_DO_L                  46.00 %          -----  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export colected statistics to LaTex table and pandas data frame\n",
    "data_frame = pd.DataFrame()\n",
    "with open(\"results/statistics_large.tex\" if LARGE_DATA_SET else \"results/statistics.tex\", \"w\") as results_file:\n",
    "    print = functools.partial(print, file=results_file)\n",
    "    row_end = \"\\\\\\\\\"\n",
    "    backslash_underscore = \"\\\\_\"\n",
    "    print(\"\\\\begin{table}[ht]\", \"\\\\tiny\", \"\\\\center\", \"\\\\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }\", sep=\"\\n\")        \n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    print(\"& \", end=\"\")\n",
    "    for header in table_header[:-1]:\n",
    "        print(f\"\\\\thead{{{header.replace(' ', row_end)}}} & \", end=\"\")\n",
    "    print(f\"\\\\thead{{{table_header[-1].replace(' ', row_end)}}} {row_end}\")\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    for model_name in model_names:\n",
    "        results_model = results[model_name]\n",
    "        print(f\"\\\\thead{{{model_name.replace('_', backslash_underscore)}}} & \", end=\"\")\n",
    "        for header in table_header[:-1]:\n",
    "            print(f\"{results_model[header]} & \", end=\"\")\n",
    "            data_frame.at[model_name, header] = results_model[header]\n",
    "        print(f\"{results_model[table_header[-1]]} {row_end}\")\n",
    "        data_frame.at[model_name, table_header[-1]] = results_model[table_header[-1]]\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\caption{Summary of analyzed models during model selection and hyperparametr tuning.}\")\n",
    "    print(\"\\\\label{table1}\")\n",
    "    print(\"\\\\end{tabular}\", \"\\\\end{table}\", sep=\"\\n\")\n",
    "\n",
    "data_frame.replace(\"\\\\\\%\", '%', regex=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
