{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - imports, constants, download of data from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras-flops in /home/david/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: tensorflow<3.0,>=2.2 in /home/david/.local/lib/python3.10/site-packages (from keras-flops) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (4.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.7.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (22.10.26)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.23.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.14.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.12.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.50.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.27.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (14.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\n",
      "Requirement already satisfied: packaging in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (21.3)\n",
      "Requirement already satisfied: setuptools in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (65.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/david/.local/lib/python3.10/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.10.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (2.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/david/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/david/.local/lib/python3.10/site-packages (from packaging->tensorflow<3.0,>=2.2->keras-flops) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/david/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/david/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/david/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/david/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/david/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/david/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<3.0,>=2.2->keras-flops) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as tfm\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow.keras.callbacks as tfc\n",
    "import tensorflow.keras.utils as tfu\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import functools\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "\n",
    "os.system(\"pip install keras-flops\")\n",
    "import keras_flops as kf\n",
    "\n",
    "SEED = 42\n",
    "IMAGE_HEIGHT = 40\n",
    "IMAGE_WIDTH = 40\n",
    "SAMPLES_PER_MEASUREMENT = 119\n",
    "LINES_PER_MEASUREMENT = SAMPLES_PER_MEASUREMENT + 1\n",
    "IMAGE_WIDTH_HEIGHT_INDEX = IMAGE_WIDTH - 1\n",
    "NUMBER_OF_LABELS = 5\n",
    "LABELS = [\"Avada Kedavra\", \"Locomotor\", \"Arresto Momentum\", \"Revelio\", \"Alohomora\"]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# create input/output directories and download data\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "if not os.path.isfile(\"data/spells.zip\"):\n",
    "    os.system(\"wget -P data/ https://github.com/xmihol00/embedded_ML/raw/main/data/spells.zip\")\n",
    "    os.system(\"unzip data/spells.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data processing, creation of data sets and model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stroke_samples(data):\n",
    "    angle_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 3))\n",
    "    stroke_samples = np.zeros((SAMPLES_PER_MEASUREMENT, 2))\n",
    "    rows_of_samples = [list(map(lambda x: float(x), line.split(','))) for line in data.split('\\n') if line]\n",
    "\n",
    "    for i in range(0, len(rows_of_samples), SAMPLES_PER_MEASUREMENT): \n",
    "        measurment = np.array(rows_of_samples[i: i+SAMPLES_PER_MEASUREMENT])\n",
    "        acceleration_average = np.average(measurment[:, 0:3], axis=0)\n",
    "\n",
    "        # calcualte angle\n",
    "        previous_angle = np.zeros(3)\n",
    "        for j, gyro_sample in enumerate(measurment[:, 3:6]):\n",
    "            angle_samples[j] = previous_angle + gyro_sample / SAMPLES_PER_MEASUREMENT\n",
    "            previous_angle = angle_samples[j]     \n",
    "        angle_avg = np.average(angle_samples, axis=0) # average angle\n",
    "\n",
    "        # calculate stroke\n",
    "        acceleration_magnitude = np.sqrt(acceleration_average.dot(acceleration_average.T)) # dot product insted of squaring\n",
    "        acceleration_magnitude += (acceleration_magnitude < 0.0001) * 0.0001 # prevent division by 0\n",
    "        normalzied_acceleration = acceleration_average / acceleration_magnitude\n",
    "        normalized_angle = angle_samples - angle_avg\n",
    "        stroke_samples[:, 0] = -normalzied_acceleration[1] * normalized_angle[:, 1] - normalzied_acceleration[2] * normalized_angle[:, 2]\n",
    "        stroke_samples[:, 1] =  normalzied_acceleration[1] * normalized_angle[:, 2] - normalzied_acceleration[2] * normalized_angle[:, 1]\n",
    "        yield stroke_samples\n",
    "\n",
    "def load_as_images(one_hot=True):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    for i, file_name in enumerate(glob.glob(\"data/*.csv\")):\n",
    "        file = open(file_name, \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    colors = np.linspace(255 - 2 * SAMPLES_PER_MEASUREMENT + 2, 255, SAMPLES_PER_MEASUREMENT) / 255\n",
    "    images = np.zeros((len(labels), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        # rasterize stroke\n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        pixels = np.round(stroke_samples * IMAGE_WIDTH_HEIGHT_INDEX / np.max(stroke_samples, axis=0), 0).astype(np.uint8) # normalize samples to the whole image\n",
    "        image = np.zeros((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        image[pixels[:, 1], pixels[:, 0]] = colors\n",
    "        images[i] = image.reshape(IMAGE_WIDTH, IMAGE_HEIGHT, 1).astype(np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = skms.train_test_split(images, labels, test_size=0.2, random_state=SEED)\n",
    "    if one_hot:\n",
    "        # one-hot encoding of labels\n",
    "        y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "\n",
    "    return X_train, X_test, y_train, np.array(y_test)\n",
    "\n",
    "def load_as_array(one_hot=True):\n",
    "    data = \"\"\n",
    "    labels = []\n",
    "    for i, file_name in enumerate(glob.glob(\"data/*.csv\")):\n",
    "        file = open(file_name, \"r\")\n",
    "        file.readline() # skip header\n",
    "        read_lines = file.read()\n",
    "        labels += [i] * (read_lines.count(\"\\n\") // LINES_PER_MEASUREMENT)\n",
    "        data += read_lines\n",
    "        file.close()\n",
    "\n",
    "    arrays = np.zeros((len(labels), 2 * SAMPLES_PER_MEASUREMENT), dtype=np.float32)\n",
    "\n",
    "    for i, stroke_samples in enumerate(get_stroke_samples(data)): \n",
    "        stroke_samples -= np.min(stroke_samples, axis=0) # make samples in range from 0 to x\n",
    "        stroke_samples /= np.max(stroke_samples, axis=0) # normalize values from 0 to 1\n",
    "        arrays[i] = stroke_samples.reshape(-1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = skms.train_test_split(arrays, labels, test_size=0.2, random_state=SEED)\n",
    "    if one_hot:\n",
    "        # one-hot encoding of labels\n",
    "        y_train = tfu.to_categorical(y_train, num_classes=5)\n",
    "\n",
    "    return X_train, X_test, y_train, np.array(y_test)\n",
    "\n",
    "def representative_dataset(data_set):\n",
    "    for sample in data_set:\n",
    "        yield [np.expand_dims(sample, 0)]\n",
    "\n",
    "def collect_model_summary(summary_line, model_dict):\n",
    "    match = re.match(r\"(.*?): ([\\d,]+)\", summary_line)\n",
    "    if match:\n",
    "        match = match.groups()\n",
    "        model_dict[match[0].replace(\"params\", \"parameters\")] = int(match[1].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_activation = tf.keras.layers.LeakyReLU(0.1)\n",
    "droput_1 = 0.4\n",
    "droput_2 = 0.3\n",
    "droput_3 = 0.25\n",
    "\n",
    "models = [\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=125, activation=hidden_activation),\n",
    "        tfl.Dense(units=75, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dense(units=128, activation=hidden_activation),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(1, 1), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=128, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(1, 1), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=100, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Dense(units=125, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=75, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\")\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=128, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(1, 1), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(5, 5), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=128, kernel_size=(3, 3), activation=hidden_activation, padding=\"valid\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(1, 1), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.Conv2D(filters=5, kernel_size=(1, 1), activation=\"softmax\", padding=\"same\"),\n",
    "        tfl.Reshape([5])\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=8, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=64, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ]),\n",
    "\n",
    "    tfm.Sequential([\n",
    "        tfl.Conv2D(filters=16, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=32, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Conv2D(filters=64, kernel_size=(3, 3), activation=hidden_activation, padding=\"same\"),\n",
    "        tfl.BatchNormalization(),\n",
    "        tfl.MaxPool2D(),\n",
    "        tfl.Flatten(),\n",
    "        tfl.Dropout(droput_1),\n",
    "        tfl.Dense(units=128, activation=hidden_activation),\n",
    "        tfl.Dropout(droput_2),\n",
    "        tfl.Dense(units=5, activation=\"softmax\"),\n",
    "    ])\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"baseline_linear\",\n",
    "    \"Only_DENS_S\", \"Only_DENS_L\",\n",
    "    \"CONV_DENS_S\", \"CONV_DENS_L\", \n",
    "    \"Only_CONV_S\", \"Only_CONV_L\",\n",
    "    \"Only_DENS_DO_S\", \"Only_DENS_DO_L\",\n",
    "    \"CONV_DENS_DO_S\", \"CONV_DENS_DO_L\",\n",
    "    \"Only_CONV_BN_S\", \"Only_CONV_BN_L\",\n",
    "    \"CONV_DENS_BN_DO_S\", \"CONV_DENS_BN_DO_L\",\n",
    "]\n",
    "\n",
    "model_data_set = [\n",
    "    0,\n",
    "    0, 0,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "    0, 0,\n",
    "    1, 1,\n",
    "    1, 1,\n",
    "    1, 1\n",
    "]\n",
    "\n",
    "table_header = [\"Total parameters\", \"Trainable parameters\", \"Non-trainable parameters\", \"Size\", \"Optimized size\", \n",
    "                \"Training time GPU\", \"Epochs\", \"FLOPS\", \"Full model accuracy\", \"Optimized model accuracy\", \"Inference time\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and collection of statistics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = [load_as_array(), load_as_images()]\n",
    "results = {}\n",
    "\n",
    "for model, model_name, data_set in zip(models, model_names, model_data_set):\n",
    "    X_train, X_test, y_train, y_test = data_sets[data_set]\n",
    "    results[model_name] = {}\n",
    "    results_model = results[model_name]\n",
    "    results_model[\"Inference time\"] = \"-----\"\n",
    "\n",
    "    # get weights for the given seed\n",
    "    model.build(X_train.shape)\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    # get the best number of epochs based on validation data set\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=2,\n",
    "                        callbacks=[tfc.EarlyStopping(monitor=\"val_accuracy\", patience=3, mode=\"max\", restore_best_weights=False)]).history\n",
    "    model.set_weights(weights)\n",
    "    epochs = len(history[\"loss\"]) - 3\n",
    "    results_model[\"Epochs\"] = epochs\n",
    "    \n",
    "    # train on the whole train data set\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    train_start = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.0, batch_size=32, verbose=2)\n",
    "    results_model[\"Training time GPU\"] = f\"{(time.time() - train_start):.2f} s\"\n",
    "\n",
    "    # predict and evaluate the prediction\n",
    "    predictions = model.predict(X_test, verbose=2)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    results_model[\"Full model accuracy\"] = f\"{((predictions == y_test).sum() / y_test.shape[0] * 100):.2f} \\\\%\"\n",
    "\n",
    "    # plot the full model confusion metrix\n",
    "    figure, axis = plt.subplots(2, 1, figsize=(12, 18))\n",
    "    figure.suptitle(f\"{model_name} confusion matrices\", fontsize=16) \n",
    "    confusion_matrix = tf.math.confusion_matrix(y_test, predictions).numpy()\n",
    "    confusion_matrix = skm.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=LABELS)\n",
    "    axis[0].set_title(f\"Full model\")\n",
    "    confusion_matrix.plot(cmap=\"Blues\", ax=axis[0])\n",
    "\n",
    "    # get the summary of the model\n",
    "    model.summary(print_fn=lambda x, y=results_model: collect_model_summary(x, y))\n",
    "    results_model[\"FLOPS\"] = kf.get_flops(model, batch_size=1)\n",
    "\n",
    "    # convert the model without optimiziation (evaluation is not necessary, the results after conversion are the same)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    results_file = open(f\"models/{model_name}.tflite\", \"wb\")\n",
    "    results_file.write(tflite_model)\n",
    "    results_file.close()\n",
    "    results_model[\"Size\"] = os.path.getsize(f\"models/{model_name}.tflite\")\n",
    "    os.system(f'echo \"const unsigned char model[] = {{\" > models/{model_name}.h && cat models/{model_name}.tflite | xxd -i >> models/{model_name}.h && echo \"}};\" >> models/{model_name}.h && rm -f models/{model_name}.tflite')\n",
    "    del tflite_model\n",
    "\n",
    "    # convert the model with optimization \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    converter.representative_dataset = lambda x=X_train: representative_dataset(x)\n",
    "    tflite_model_opt = converter.convert()\n",
    "    results_file = open(f\"models/{model_name}.tflite\", \"wb\")\n",
    "    results_file.write(tflite_model_opt)\n",
    "    results_file.close()\n",
    "    results_model[\"Optimized size\"] = os.path.getsize(f\"models/{model_name}.tflite\")\n",
    "    os.system(f'echo \"const unsigned char model[] = {{\" > models/{model_name}_opt.h && cat models/{model_name}.tflite | xxd -i >> models/{model_name}_opt.h && echo \"}};\" >> models/{model_name}_opt.h && rm -f models/{model_name}.tflite')\n",
    "\n",
    "    # predict using the optimized model and evaluate the prediction\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model_opt)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    input_scale, input_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "    predictions = np.zeros((y_test.shape[0]))\n",
    "    for i, sample in enumerate(X_test):\n",
    "        interpreter.set_tensor(input_index, np.expand_dims(sample / input_scale + input_zero_point, 0).astype(np.int8))\n",
    "        interpreter.invoke()\n",
    "        predictions[i] = np.argmax(interpreter.get_tensor(output_index)[0]) # rescaling is not needed\n",
    "    results_model[\"Optimized model accuracy\"] = f\"{((predictions == y_test).sum() / y_test.shape[0] * 100):.2f} \\\\%\"\n",
    "    \n",
    "    # plot the confusuion matrix of the optimized model\n",
    "    confusion_matrix = tf.math.confusion_matrix(y_test, predictions).numpy()\n",
    "    confusion_matrix = skm.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=LABELS)\n",
    "    axis[1].set_title(f\"Optimized model\")\n",
    "    confusion_matrix.plot(cmap=\"Blues\", ax=axis[1])\n",
    "    plt.savefig(f\"figures/{model_name}_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "    del tflite_model_opt\n",
    "\n",
    "    # clear cell output after each model\n",
    "    ipd.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total parameters</th>\n",
       "      <th>Trainable parameters</th>\n",
       "      <th>Non-trainable parameters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Optimized size</th>\n",
       "      <th>Training time GPU</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>FLOPS</th>\n",
       "      <th>Full model accuracy</th>\n",
       "      <th>Optimized model accuracy</th>\n",
       "      <th>Inference time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_linear</th>\n",
       "      <td>1195.0</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6100.0</td>\n",
       "      <td>2736.0</td>\n",
       "      <td>0.25 s</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2410.0</td>\n",
       "      <td>86.00 %</td>\n",
       "      <td>83.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_S</th>\n",
       "      <td>24405.0</td>\n",
       "      <td>24405.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99520.0</td>\n",
       "      <td>26968.0</td>\n",
       "      <td>0.41 s</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48730.0</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_L</th>\n",
       "      <td>39705.0</td>\n",
       "      <td>39705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161284.0</td>\n",
       "      <td>43280.0</td>\n",
       "      <td>0.38 s</td>\n",
       "      <td>12.0</td>\n",
       "      <td>79230.0</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_S</th>\n",
       "      <td>57477.0</td>\n",
       "      <td>57477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234748.0</td>\n",
       "      <td>64864.0</td>\n",
       "      <td>0.83 s</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2221534.0</td>\n",
       "      <td>96.00 %</td>\n",
       "      <td>96.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_L</th>\n",
       "      <td>228869.0</td>\n",
       "      <td>228869.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>920316.0</td>\n",
       "      <td>237960.0</td>\n",
       "      <td>0.82 s</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8334238.0</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_S</th>\n",
       "      <td>26757.0</td>\n",
       "      <td>26757.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113504.0</td>\n",
       "      <td>38368.0</td>\n",
       "      <td>0.84 s</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1512638.0</td>\n",
       "      <td>93.00 %</td>\n",
       "      <td>93.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_L</th>\n",
       "      <td>105989.0</td>\n",
       "      <td>105989.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>430432.0</td>\n",
       "      <td>121704.0</td>\n",
       "      <td>1.15 s</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4950366.0</td>\n",
       "      <td>94.00 %</td>\n",
       "      <td>95.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_DO_S</th>\n",
       "      <td>24405.0</td>\n",
       "      <td>24405.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99520.0</td>\n",
       "      <td>26968.0</td>\n",
       "      <td>0.23 s</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48730.0</td>\n",
       "      <td>84.00 %</td>\n",
       "      <td>84.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_DENS_DO_L</th>\n",
       "      <td>39705.0</td>\n",
       "      <td>39705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161284.0</td>\n",
       "      <td>43280.0</td>\n",
       "      <td>0.28 s</td>\n",
       "      <td>3.0</td>\n",
       "      <td>79230.0</td>\n",
       "      <td>90.00 %</td>\n",
       "      <td>89.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_DO_S</th>\n",
       "      <td>57477.0</td>\n",
       "      <td>57477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234748.0</td>\n",
       "      <td>64864.0</td>\n",
       "      <td>0.44 s</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2221534.0</td>\n",
       "      <td>86.00 %</td>\n",
       "      <td>86.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_DO_L</th>\n",
       "      <td>228869.0</td>\n",
       "      <td>228869.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>920316.0</td>\n",
       "      <td>237960.0</td>\n",
       "      <td>1.25 s</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8334238.0</td>\n",
       "      <td>98.00 %</td>\n",
       "      <td>98.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_BN_S</th>\n",
       "      <td>27365.0</td>\n",
       "      <td>27061.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>117864.0</td>\n",
       "      <td>42520.0</td>\n",
       "      <td>0.61 s</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1544974.0</td>\n",
       "      <td>41.00 %</td>\n",
       "      <td>41.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Only_CONV_BN_L</th>\n",
       "      <td>107205.0</td>\n",
       "      <td>106597.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>436008.0</td>\n",
       "      <td>126160.0</td>\n",
       "      <td>0.53 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5015038.0</td>\n",
       "      <td>19.00 %</td>\n",
       "      <td>21.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_BN_DO_S</th>\n",
       "      <td>57701.0</td>\n",
       "      <td>57589.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>237096.0</td>\n",
       "      <td>67312.0</td>\n",
       "      <td>0.42 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2266670.0</td>\n",
       "      <td>57.00 %</td>\n",
       "      <td>55.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONV_DENS_BN_DO_L</th>\n",
       "      <td>229317.0</td>\n",
       "      <td>229093.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>923112.0</td>\n",
       "      <td>240520.0</td>\n",
       "      <td>0.47 s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8424510.0</td>\n",
       "      <td>76.00 %</td>\n",
       "      <td>75.00 %</td>\n",
       "      <td>-----</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Total parameters  Trainable parameters  \\\n",
       "baseline_linear              1195.0                1195.0   \n",
       "Only_DENS_S                 24405.0               24405.0   \n",
       "Only_DENS_L                 39705.0               39705.0   \n",
       "CONV_DENS_S                 57477.0               57477.0   \n",
       "CONV_DENS_L                228869.0              228869.0   \n",
       "Only_CONV_S                 26757.0               26757.0   \n",
       "Only_CONV_L                105989.0              105989.0   \n",
       "Only_DENS_DO_S              24405.0               24405.0   \n",
       "Only_DENS_DO_L              39705.0               39705.0   \n",
       "CONV_DENS_DO_S              57477.0               57477.0   \n",
       "CONV_DENS_DO_L             228869.0              228869.0   \n",
       "Only_CONV_BN_S              27365.0               27061.0   \n",
       "Only_CONV_BN_L             107205.0              106597.0   \n",
       "CONV_DENS_BN_DO_S           57701.0               57589.0   \n",
       "CONV_DENS_BN_DO_L          229317.0              229093.0   \n",
       "\n",
       "                   Non-trainable parameters      Size  Optimized size  \\\n",
       "baseline_linear                         0.0    6100.0          2736.0   \n",
       "Only_DENS_S                             0.0   99520.0         26968.0   \n",
       "Only_DENS_L                             0.0  161284.0         43280.0   \n",
       "CONV_DENS_S                             0.0  234748.0         64864.0   \n",
       "CONV_DENS_L                             0.0  920316.0        237960.0   \n",
       "Only_CONV_S                             0.0  113504.0         38368.0   \n",
       "Only_CONV_L                             0.0  430432.0        121704.0   \n",
       "Only_DENS_DO_S                          0.0   99520.0         26968.0   \n",
       "Only_DENS_DO_L                          0.0  161284.0         43280.0   \n",
       "CONV_DENS_DO_S                          0.0  234748.0         64864.0   \n",
       "CONV_DENS_DO_L                          0.0  920316.0        237960.0   \n",
       "Only_CONV_BN_S                        304.0  117864.0         42520.0   \n",
       "Only_CONV_BN_L                        608.0  436008.0        126160.0   \n",
       "CONV_DENS_BN_DO_S                     112.0  237096.0         67312.0   \n",
       "CONV_DENS_BN_DO_L                     224.0  923112.0        240520.0   \n",
       "\n",
       "                  Training time GPU  Epochs      FLOPS Full model accuracy  \\\n",
       "baseline_linear              0.25 s     6.0     2410.0             86.00 %   \n",
       "Only_DENS_S                  0.41 s    19.0    48730.0             94.00 %   \n",
       "Only_DENS_L                  0.38 s    12.0    79230.0             95.00 %   \n",
       "CONV_DENS_S                  0.83 s    10.0  2221534.0             96.00 %   \n",
       "CONV_DENS_L                  0.82 s     6.0  8334238.0             95.00 %   \n",
       "Only_CONV_S                  0.84 s    11.0  1512638.0             93.00 %   \n",
       "Only_CONV_L                  1.15 s    11.0  4950366.0             94.00 %   \n",
       "Only_DENS_DO_S               0.23 s     2.0    48730.0             84.00 %   \n",
       "Only_DENS_DO_L               0.28 s     3.0    79230.0             90.00 %   \n",
       "CONV_DENS_DO_S               0.44 s     3.0  2221534.0             86.00 %   \n",
       "CONV_DENS_DO_L               1.25 s    10.0  8334238.0             98.00 %   \n",
       "Only_CONV_BN_S               0.61 s     3.0  1544974.0             41.00 %   \n",
       "Only_CONV_BN_L               0.53 s     1.0  5015038.0             19.00 %   \n",
       "CONV_DENS_BN_DO_S            0.42 s     1.0  2266670.0             57.00 %   \n",
       "CONV_DENS_BN_DO_L            0.47 s     1.0  8424510.0             76.00 %   \n",
       "\n",
       "                  Optimized model accuracy Inference time  \n",
       "baseline_linear                    83.00 %          -----  \n",
       "Only_DENS_S                        94.00 %          -----  \n",
       "Only_DENS_L                        95.00 %          -----  \n",
       "CONV_DENS_S                        96.00 %          -----  \n",
       "CONV_DENS_L                        95.00 %          -----  \n",
       "Only_CONV_S                        93.00 %          -----  \n",
       "Only_CONV_L                        95.00 %          -----  \n",
       "Only_DENS_DO_S                     84.00 %          -----  \n",
       "Only_DENS_DO_L                     89.00 %          -----  \n",
       "CONV_DENS_DO_S                     86.00 %          -----  \n",
       "CONV_DENS_DO_L                     98.00 %          -----  \n",
       "Only_CONV_BN_S                     41.00 %          -----  \n",
       "Only_CONV_BN_L                     21.00 %          -----  \n",
       "CONV_DENS_BN_DO_S                  55.00 %          -----  \n",
       "CONV_DENS_BN_DO_L                  75.00 %          -----  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export colected statistics to LaTex table and pandas data frame\n",
    "data_frame = pd.DataFrame()\n",
    "with open(\"results/statistics.tex\", \"w\") as results_file:\n",
    "    print = functools.partial(print, file=results_file)\n",
    "    row_end = \"\\\\\\\\\"\n",
    "    backslash_underscore = \"\\\\_\"\n",
    "    print(\"\\\\begin{table}[ht]\", \"\\\\tiny\", \"\\\\center\", \"\\\\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| }\", sep=\"\\n\")        \n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    print(\"& \", end=\"\")\n",
    "    for header in table_header[:-1]:\n",
    "        print(f\"\\\\thead{{{header.replace(' ', row_end)}}} & \", end=\"\")\n",
    "    print(f\"\\\\thead{{{table_header[-1].replace(' ', row_end)}}} {row_end}\")\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    for model_name in model_names:\n",
    "        results_model = results[model_name]\n",
    "        print(f\"\\\\thead{{{model_name.replace('_', backslash_underscore)}}} & \", end=\"\")\n",
    "        for header in table_header[:-1]:\n",
    "            print(f\"{results_model[header]} & \", end=\"\")\n",
    "            data_frame.at[model_name, header] = results_model[header]\n",
    "        print(f\"{results_model[table_header[-1]]} {row_end}\")\n",
    "        data_frame.at[model_name, table_header[-1]] = results_model[table_header[-1]]\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\end{tabular}\", \"\\\\end{table}\", sep=\"\\n\")\n",
    "\n",
    "data_frame.replace(\"\\\\\\%\", '%', regex=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
